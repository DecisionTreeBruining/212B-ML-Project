{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1238c4de-bef0-427c-a31c-17418e99f09f",
   "metadata": {},
   "source": [
    "# Multi-Perceptron Grid_Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e073e2-05b2-4fc6-9574-de83ef7c7f35",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074514d8-c5dc-46e5-ac94-9efaf39acdad",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Notebook Setup](#Notebook-Setup)\n",
    "- [MLP GridSearch](#MLP-GridSearch)\n",
    "- [MLP Parameters](#MLP-Parameters)\n",
    "- [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53820c5-2013-433a-b156-864143ef2b64",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Significant functions from [assignment_3_tools.py](./assignment_3_tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cb0916-cef7-4ebd-857c-814f6db863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Runtime\n",
    "import pickle # Model Saving\n",
    "import logging # Log Checkpoints\n",
    "import numpy as np # Flatten y vectors\n",
    "import pandas as pd # DataFrame\n",
    "import polars as pl # LazyFrame\n",
    "from sklearn.preprocessing import StandardScaler # X Standardization\n",
    "from sklearn.neural_network import MLPClassifier as mlp # model\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score # Scoring\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from great_tables import GT, md, html, from_column, style, loc, vals\n",
    "from assignment_3_tools import parquet_to_dict, unq_df_names, corr_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cedacb-a494-4f84-8bad-42e9e86c411e",
   "metadata": {},
   "source": [
    "## Unique Datasets and Corresponding Testsets\n",
    "\n",
    "In [preprocess notebook](./taylor_preprocess.ipynb), all of the null-threshold datasets were split into X_train, y_train, X_test, and y_test. The X_train, and y_train sets of each null-threshold datasets were balanced using random over/under sampling. Therefore when `parquet_to_dict()` is called, the dictionary will contain the X_train, y_train, X_test, y_test which correspond to one dataset. To resolve this, `unq_df_names()` and `corr_testset` record the dataset names and corresponding testsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e2525-b076-4432-8f3e-0303ffab4f27",
   "metadata": {},
   "source": [
    "## MLP Baseline Model Loop\n",
    "\n",
    "This MLP model loops through all of the unique dataset names from `unq_df_names` and trains an MLP model on each unique dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b369f3-cf02-449e-83d4-fd022340e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_gridsearch(lazy_dict, unq_names, param_grid, save_pth, test_name, threads=None):\n",
    "    \"\"\"\n",
    "    MLP GridSearch using 5-fold Cross Validation. Saves best model and results.\n",
    "    ---\n",
    "    Args:\n",
    "        lazy_dict: Dictionary with names and LazyFrames of train and test sets.\n",
    "        unq_names: List of unique names of parent datasets.\n",
    "        param_grid: Dictionary of parameters for MLPClassifier.\n",
    "        save_pth: String of the path to save the best model.\n",
    "        test_name: String of the test performed.\n",
    "        threads: Integer of CPU threads for cross-validation (optional).\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ## Initializing\n",
    "    # Define number of threads to be used in GridSearch\n",
    "    if threads is None:\n",
    "        threads = os.cpu_count() - 2\n",
    "        print(f\"Using {threads} CPU threads!\")\n",
    "\n",
    "    # Log for debugging\n",
    "    logging.basicConfig(\n",
    "        filename=f\"./log/MLP_{test_name}.log\",\n",
    "        filemode='w', \n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    ## GridSearch and Results\n",
    "    for name in unq_names:\n",
    "        # Results from prediction on test_set\n",
    "        best_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"ROC_AUC\": [], \n",
    "            \"Accuracy\": [],\n",
    "            \"Fit_Time\": []}\n",
    "        \n",
    "        # Results from prediction on Cross Validation Set\n",
    "        param_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"Fit_Time\": []}\n",
    "        \n",
    "        ## Reading and Preparing Data\n",
    "        # Dataset names in path\n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        X_test_name = f\"{name}_X_test\"\n",
    "        y_test_name = f\"{name}_y_test\"\n",
    "\n",
    "        # Train and test sets.\n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_pandas()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_pandas()\n",
    "\n",
    "        # Drop index column\n",
    "        X_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        X_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "\n",
    "        # Flatten response sets\n",
    "        y_train = y_train.to_numpy().ravel()\n",
    "        y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "        # Standardize predictor sets\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        ## Defining Modeling and GridSearch\n",
    "        # Define cross-validation folds\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=212)\n",
    "\n",
    "        # Define mlp model\n",
    "        mlp_model = mlp(early_stopping=True, random_state=212)\n",
    "\n",
    "        # Define GridSearch\n",
    "        grid_search = GridSearchCV(\n",
    "            mlp_model,\n",
    "            param_grid=param_grid, \n",
    "            cv=cv,\n",
    "            scoring='recall',\n",
    "            n_jobs=threads,\n",
    "            verbose=1, \n",
    "            return_train_score=True)\n",
    "\n",
    "\n",
    "        ## Performing GridSearch\n",
    "        # Debugging Checkpoint\n",
    "        logging.info(f\"Processing dataset: {name}\")\n",
    "        print(f\"Training on {name}...\", flush=True)\n",
    "\n",
    "        # GridSearch Training and Results\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        print(f\"GridSearch completed\", flush=True)\n",
    "        logging.info(f\"GridSearch for {test_name} completed.\")\n",
    "\n",
    "        ## Results from GridSearch\n",
    "        # Storing Results for each parameter combination\n",
    "        for i in range(len(grid_search.cv_results_['params'])):\n",
    "            param_combination = grid_search.cv_results_['params'][i]\n",
    "            recall = grid_search.cv_results_['mean_test_score'][i]\n",
    "            fit_time = grid_search.cv_results_['mean_fit_time'][i]\n",
    "            param_results[\"Dataset_Name\"].append(name)\n",
    "            param_results[\"Grid_Variable\"].append(test_name)\n",
    "            param_results[\"Parameters\"].append(param_combination)\n",
    "            param_results[\"Recall\"].append(recall)\n",
    "            param_results[\"Fit_Time\"].append(fit_time)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        param_results_df = pd.DataFrame(param_results)\n",
    "        param_results_df = param_results_df.sort_values(by=\"Recall\", ascending=False)\n",
    "\n",
    "        # Best model by Recall on Cross Validation data\n",
    "        best_fit_time = param_results_df.iloc[0][\"Fit_Time\"]\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Metrics on test set\n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        test_recall = recall_score(y_test, y_pred_test)\n",
    "        test_roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        # Save best model as pickle\n",
    "        with open(f\"{save_pth}best_model_{test_name}{name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(best_model, file)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        logging.info(f\"Model saved to {save_pth}\")\n",
    "\n",
    "        # Results from predicting test data using the best model.\n",
    "        best_results[\"Dataset_Name\"].append(name)\n",
    "        best_results[\"Grid_Variable\"].append(test_name)\n",
    "        best_results[\"Parameters\"].append(grid_search.best_params_)\n",
    "        best_results[\"Recall\"].append(test_recall)\n",
    "        best_results[\"ROC_AUC\"].append(test_roc_auc)\n",
    "        best_results[\"Accuracy\"].append(test_accuracy)\n",
    "        best_results[\"Fit_Time\"].append(best_fit_time)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        best_results_df = pd.DataFrame(best_results)\n",
    "\n",
    "        # Save results as Parquet\n",
    "        best_results_df.to_parquet(f\"{save_pth}test_results_{test_name}_{name}.parquet\", index=False)\n",
    "        param_results_df.to_parquet(f\"{save_pth}grid_results_{test_name}_{name}.parquet\", index=False)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        print(f\"{test_name} GridSearch completed!\", flush=True)\n",
    "        logging.info(f\"{test_name} GridSearch completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3237d8-b0be-4054-abcf-78889304635d",
   "metadata": {},
   "source": [
    "## MLP Parameters\n",
    "\n",
    "Change the `param_grid` and `test_name` to match the test being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495da1b2-e6fd-4877-92ca-4935bd12148d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and test sets are in MLP_Dataset.\n",
    "# Save results and best model to MLP_Results.\n",
    "data_pth = \"../../../Data/GoogleDrive/MLP_Dataset/\"\n",
    "save_pth = \"../../../Data/GoogleDrive/MLP_Results/\"\n",
    "\n",
    "# Read in Parquet files in path and add to a LazyFrame dictionary.\n",
    "pq_jar = parquet_to_dict(data_pth)\n",
    "\n",
    "# Record the unique dataset names (drop X_train, etc.).\n",
    "unq_names = unq_df_names(pq_jar)\n",
    "\n",
    "# Baseline sklearn mlp_classification parameters.\n",
    "# These are all of the parameters we are going to test/are different from default.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,)],  # Single layer with 100 neurons\n",
    "    'activation': ['relu'],  # Using 'relu' activation function\n",
    "    'solver': ['adam'],  # Solver set to 'adam'\n",
    "    'alpha': [0.0001],  # L2 penalty (regularization term)\n",
    "    'batch_size': ['auto'],  # 'auto' sets batch size to min(200, n_samples)\n",
    "    'learning_rate': ['constant'],  # Learning rate schedule\n",
    "    'learning_rate_init': [0.001],  # Initial learning rate\n",
    "    'max_iter': [200],  # Maximum number of iterations\n",
    "    'momentum': [0.9],  # Momentum for gradient descent update\n",
    "    'n_iter_no_change': [10],  # Maximum number of epochs to not meet improvement threshold\n",
    "}\n",
    "\n",
    "test_name = \"baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd095a-39ec-4a3c-894a-5dd8e18f8f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 CPU threads!\n",
      "Training on Under_Sample_1:1_threshold_20_Yes...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = mlp_gridsearch(pq_jar, unq_names, param_grid, save_pth, test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ee79b-40e1-40ce-b8c3-339d3833a5ab",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9117de-dd18-479d-a950-064b30e9606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in Results\n",
    "\n",
    "# Best CV model by Recall\n",
    "with open(f\"{save_pth}best_model_{test_name}.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Test set prediction results\n",
    "results_df = pd.read_parquet(f\"{save_pth}test_results_{test_name}.parquet\")\n",
    "\n",
    "# Cross Validation results\n",
    "grid_df = pd.read_parquet(f\"{save_pth}grid_results_{test_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18022e1-850a-4a0f-b740-a48431ee4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set results\n",
    "(\n",
    "    GT(results_df.sort_values(by='Recall', ascending=False))\n",
    "    .fmt_number(columns=[\"Recall\",\"ROC_AUC\",\"Accuracy\"], decimals=2)\n",
    "    .fmt_number(columns=[\"Fit_Time\"], decimals=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951810f-85be-4050-9ecd-6945c1702749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Results\n",
    "(\n",
    "    GT(grid_df.sort_values(by='Recall', ascending=False))\n",
    "    .fmt_number(columns=[\"Recall\"], decimals=2)\n",
    "    .fmt_number(columns=[\"Fit_Time\"], decimals=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f183f8d-28b1-4dc5-85cb-57ddc29ce9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = parquet_to_dict(\"../../../Data/GoogleDrive/MLP_Results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e9ea9-ba9b-4cce-bc02-1e1144d5cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq[\"test_results_baseline_Under_Sample_1:1_threshold_20_Yes\"].collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a06205-73b0-4a4d-8133-99f15ece430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq[\"test_results_baseline_Under_Sample_1:1_threshold_20_No\"].collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b7a65-64b4-45fe-972d-1b66e82af18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq[\"test_results_baseline_Under_Sample_1:1_threshold_20\"].collect().to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "203C",
   "language": "python",
   "name": "203c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
