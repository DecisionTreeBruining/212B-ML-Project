{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frankenstein Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Notebook Setup](#Notebook-Setup)\n",
    "- [MLP GridSearch](#MLP-GridSearch)\n",
    "- [MLP Parameters](#MLP-Parameters)\n",
    "- [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Significant functions from [assignment_3_tools.py](./assignment_3_tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # List \n",
    "import time # Runtime\n",
    "import pickle # Model Saving\n",
    "import logging # Log Checkpoints\n",
    "import numpy as np # Flatten y vectors\n",
    "import pandas as pd # DataFrame\n",
    "import polars as pl # LazyFrame\n",
    "from sklearn.preprocessing import StandardScaler # X Standardization\n",
    "from sklearn.neural_network import MLPClassifier as mlp # model\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score, auc, roc_curve  # Scoring\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from great_tables import GT, md, html, from_column, style, loc, vals\n",
    "from assignment_3_tools import parquet_to_dict, unq_df_names, corr_testset\n",
    "import xgboost as xgb\n",
    "# add scikit optimize for bayesian optimization\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_gridsearch(lazy_dict, unq_names, param_grid, save_pth, test_name, threads=None):\n",
    "    \"\"\"\n",
    "    MLP GridSearch using 5-fold Cross Validation. Saves best model and results.\n",
    "    ---\n",
    "    Args:\n",
    "        lazy_dict: Dictionary with names and LazyFrames of train and test sets.\n",
    "        unq_names: List of unique names of parent datasets.\n",
    "        param_grid: Dictionary of parameters for MLPClassifier. CHANGE FOR BAESIAN\n",
    "        save_pth: String of the path to save the best model.\n",
    "        test_name: String of the test performed. PARAMETER BEING TESTED\n",
    "        threads: Integer of CPU threads for cross-validation (optional).\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ## Initializing\n",
    "    # Define number of threads to be used in GridSearch\n",
    "    if threads is None:\n",
    "        threads = os.cpu_count() - 4\n",
    "        print(f\"Using {threads} CPU threads!\")\n",
    "\n",
    "    # Log for debugging\n",
    "    logging.basicConfig(\n",
    "        filename=f\"./log/MLP_{test_name}.log\",\n",
    "        filemode='w', \n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    ## GridSearch and Results\n",
    "    for name in unq_names:\n",
    "        # Results from prediction on test_set. FOR TEST TABLE\n",
    "        best_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"ROC_AUC\": [], \n",
    "            \"Accuracy\": [],\n",
    "            \"Fit_Time\": []}\n",
    "        \n",
    "        # Results from prediction on Cross Validation Set. FOR CV TABLE\n",
    "        param_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"Fit_Time\": []}\n",
    "        \n",
    "        ## Reading and Preparing Data\n",
    "        # Dataset names in path\n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        X_test_name = f\"{name}_X_test\"\n",
    "        y_test_name = f\"{name}_y_test\"\n",
    "\n",
    "        # Train and test sets.\n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_pandas()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_pandas()\n",
    "\n",
    "        # Drop index column\n",
    "        X_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        X_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "\n",
    "        # Flatten response sets\n",
    "        y_train = y_train.to_numpy().ravel()\n",
    "        y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "        # Standardize predictor sets\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        ## Defining Modeling and GridSearch. CHANGE FOR BEASIAN\n",
    "        # Define cross-validation folds\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=212)\n",
    "\n",
    "        # Define mlp model\n",
    "        mlp_model = mlp()\n",
    "\n",
    "        # Define GridSearch. CHANGE TO BEASIAN!!!\n",
    "        grid_search = GridSearchCV(\n",
    "            mlp_model, #mlp model\n",
    "            param_grid=param_grid, #parameter dictionary\n",
    "            cv=cv, # cv\n",
    "            scoring='recall', # best is by recall\n",
    "            n_jobs=threads,\n",
    "            verbose=3, \n",
    "            return_train_score=True) # For making CV table\n",
    "\n",
    "\n",
    "        ## Performing GridSearch\n",
    "        # Debugging Checkpoint\n",
    "        logging.info(f\"Processing dataset: {name}\")\n",
    "        print(f\"Training on {name}...\", flush=True)\n",
    "\n",
    "        # GridSearch Training and Results\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        print(f\"GridSearch completed\", flush=True)\n",
    "        logging.info(f\"GridSearch for {test_name} completed.\")\n",
    "\n",
    "        ## Results from GridSearch\n",
    "        # Storing Results for each parameter combination\n",
    "        for i in range(len(grid_search.cv_results_['params'])):\n",
    "            param_combination = grid_search.cv_results_['params'][i]\n",
    "            recall = grid_search.cv_results_['mean_test_score'][i]\n",
    "            fit_time = grid_search.cv_results_['mean_fit_time'][i]\n",
    "            param_results[\"Dataset_Name\"].append(name)\n",
    "            param_results[\"Grid_Variable\"].append(test_name)\n",
    "            param_results[\"Parameters\"].append(param_combination)\n",
    "            param_results[\"Recall\"].append(recall)\n",
    "            param_results[\"Fit_Time\"].append(fit_time)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        param_results_df = pd.DataFrame(param_results)\n",
    "        param_results_df = param_results_df.sort_values(by=\"Recall\", ascending=False)\n",
    "\n",
    "        # Best model by Recall on Cross Validation data\n",
    "        best_fit_time = param_results_df.iloc[0][\"Fit_Time\"]\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Metrics on test set\n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        test_recall = recall_score(y_test, y_pred_test)\n",
    "        test_roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        # Save best model as pickle\n",
    "        # example: best_model_neurons-hidden_layer_sizes.pkl\n",
    "        with open(f\"{save_pth}best_model{test_name}-{name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(best_model, file)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        logging.info(f\"Model saved to {save_pth}\")\n",
    "\n",
    "        # Results from predicting test data using the best model.\n",
    "        best_results[\"Dataset_Name\"].append(name)\n",
    "        best_results[\"Grid_Variable\"].append(test_name)\n",
    "        best_results[\"Parameters\"].append(grid_search.best_params_)\n",
    "        best_results[\"Recall\"].append(test_recall)\n",
    "        best_results[\"ROC_AUC\"].append(test_roc_auc)\n",
    "        best_results[\"Accuracy\"].append(test_accuracy)\n",
    "        best_results[\"Fit_Time\"].append(best_fit_time)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        best_results_df = pd.DataFrame(best_results)\n",
    "        \n",
    "        # Save results as Parquet\n",
    "        # example: test_results_layers-hidden_layer_sizes.parquet\n",
    "        best_results_df.to_parquet(f\"{save_pth}test_results{test_name}-{name}.parquet\", index=False)\n",
    "        # example: grid_results_neurons-hidden_layer_sizes.parquet\n",
    "        param_results_df.to_parquet(f\"{save_pth}grid_results{test_name}-{name}.parquet\", index=False)\n",
    "\n",
    "        # Debugging Checkpoint\n",
    "        print(f\"{test_name} GridSearch completed!\", flush=True)\n",
    "        logging.info(f\"{test_name} GridSearch completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Parameters\n",
    "\n",
    "Change the `param_grid` and `test_name` to match the test being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets are in MLP_Dataset.\n",
    "# Save results and best model to MLP_Results.\n",
    "data_pth = \"../../../Data/GoogleDrive/MLP_Dataset/\"\n",
    "save_pth = \"../../../Data/GoogleDrive/MLP_Results/\"\n",
    "\n",
    "# Read in Parquet files in path and add to a LazyFrame dictionary.\n",
    "pq_jar = parquet_to_dict(data_pth) # all lazy\n",
    "\n",
    "# Record the unique dataset names (drop X_train, etc.).\n",
    "unq_names = unq_df_names(pq_jar)\n",
    "\n",
    "# A dictionary of parmeter dictionaries\n",
    "# Schema {testname:{parameter:values}}\n",
    "\n",
    "all_test_parameters = {\n",
    "    '_best_params':{\n",
    "        'hidden_layer_sizes': [(47, 46, 46, 46)],\n",
    "        'learning_rate_init': [0.01]}\n",
    "}\n",
    "\n",
    "frank_param = {\n",
    "    '_best_params_vem_2':{\n",
    "        'hidden_layer_sizes': [(47, 46, 46, 46)],  # Number of neurons and layers, represented as a tuple\n",
    "        'activation': ['relu'],         # Activation function\n",
    "        'alpha': [0.0001],              # Regularization parameter\n",
    "        'batch_size': ['auto'],         # Size of minibatches\n",
    "        'learning_rate': ['constant'],  # Learning rate schedule\n",
    "        'learning_rate_init': [0.01],   # Initial learning rate\n",
    "        'max_iter': [200],              # Maximum number of iterations\n",
    "        'momentum': [0.9],              # Momentum for gradient descent\n",
    "        'n_iter_no_change': [10],       # Number of iterations with no improvement to stop training\n",
    "        'solver': ['adam']              # Solver for weight optimization\n",
    "}\n",
    "}\n",
    "\n",
    "test = '_best_params_vem_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 CPU threads!\n",
      "Training on Under_Sample_1:1_threshold_20...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 5/5] END hidden_layer_sizes=(47, 46, 46, 46), learning_rate_init=0.01;, score=(train=0.880, test=0.871) total time= 1.8min\n",
      "[CV 1/5] END hidden_layer_sizes=(47, 46, 46, 46), learning_rate_init=0.01;, score=(train=0.933, test=0.927) total time= 2.7min\n",
      "[CV 3/5] END hidden_layer_sizes=(47, 46, 46, 46), learning_rate_init=0.01;, score=(train=0.915, test=0.908) total time= 2.8min\n",
      "[CV 2/5] END hidden_layer_sizes=(47, 46, 46, 46), learning_rate_init=0.01;, score=(train=0.947, test=0.939) total time= 2.8min\n",
      "[CV 4/5] END hidden_layer_sizes=(47, 46, 46, 46), learning_rate_init=0.01;, score=(train=0.928, test=0.920) total time= 3.4min\n",
      "GridSearch completed\n",
      "_best_params GridSearch completed!\n",
      "CPU times: user 1h 42min 13s, sys: 3min 23s, total: 1h 45min 36s\n",
      "Wall time: 12min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the model\n",
    "for test, param_dict in all_test_parameters.items():\n",
    "    mlp_gridsearch(pq_jar, unq_names, param_dict, save_pth, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 CPU threads!\n",
      "Training on Under_Sample_1:1_threshold_20...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 2/5] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(47, 46, 46, 46), learning_rate=constant, learning_rate_init=0.01, max_iter=200, momentum=0.9, n_iter_no_change=10, solver=adam;, score=(train=0.956, test=0.948) total time= 2.5min\n",
      "[CV 1/5] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(47, 46, 46, 46), learning_rate=constant, learning_rate_init=0.01, max_iter=200, momentum=0.9, n_iter_no_change=10, solver=adam;, score=(train=0.804, test=0.794) total time= 2.5min\n",
      "[CV 4/5] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(47, 46, 46, 46), learning_rate=constant, learning_rate_init=0.01, max_iter=200, momentum=0.9, n_iter_no_change=10, solver=adam;, score=(train=0.937, test=0.929) total time= 2.5min\n",
      "[CV 5/5] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(47, 46, 46, 46), learning_rate=constant, learning_rate_init=0.01, max_iter=200, momentum=0.9, n_iter_no_change=10, solver=adam;, score=(train=0.929, test=0.921) total time= 2.6min\n",
      "[CV 3/5] END activation=relu, alpha=0.0001, batch_size=auto, hidden_layer_sizes=(47, 46, 46, 46), learning_rate=constant, learning_rate_init=0.01, max_iter=200, momentum=0.9, n_iter_no_change=10, solver=adam;, score=(train=0.949, test=0.940) total time= 3.5min\n",
      "GridSearch completed\n",
      "_best_params_vem_2 GridSearch completed!\n",
      "CPU times: user 2h 8min 58s, sys: 3min 56s, total: 2h 12min 54s\n",
      "Wall time: 15min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the model\n",
    "for test, param_dict in frank_param.items():\n",
    "    mlp_gridsearch(pq_jar, unq_names, param_dict, save_pth, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in Results\n",
    "\n",
    "name = 'best_params-Under_Sample_1:1_threshold_20'\n",
    "\n",
    "# Best model by Recall\n",
    "with open(f\"{save_pth}best_model_{name}.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Test set prediction results\n",
    "results_df = pd.read_parquet(f\"{save_pth}test_results_{name}.parquet\")\n",
    "\n",
    "# Cross Validation results\n",
    "grid_df = pd.read_parquet(f\"{save_pth}grid_results_{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.811796</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.691627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CV</td>\n",
       "      <td>0.912884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Result_Type    Recall   ROC_AUC  Accuracy\n",
       "0        Test  0.811796  0.814387  0.691627\n",
       "0          CV  0.912884       NaN       NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data type of the model\n",
    "frank_results_df = pd.concat([results_df, grid_df], axis=0)\n",
    "frank_results_df = frank_results_df[['Recall', \n",
    "                                     'ROC_AUC', \n",
    "                                       'Accuracy']]\n",
    "frank_results_df['Result_Type'] = ['Test'] * len(results_df) + ['CV'] * len(grid_df)\n",
    "frank_results_df = frank_results_df[['Result_Type', 'Recall', 'ROC_AUC', 'Accuracy']]\n",
    "frank_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in Results\n",
    "\n",
    "name = 'best_params_vem_2-Under_Sample_1:1_threshold_20'\n",
    "\n",
    "# Best model by Recall\n",
    "with open(f\"{save_pth}best_model_{name}.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Test set prediction results\n",
    "results_df_2 = pd.read_parquet(f\"{save_pth}test_results_{name}.parquet\")\n",
    "\n",
    "# Cross Validation results\n",
    "grid_df_2 = pd.read_parquet(f\"{save_pth}grid_results_{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.694148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CV</td>\n",
       "      <td>0.906449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Result_Type    Recall   ROC_AUC  Accuracy\n",
       "0        Test  0.815167  0.815958  0.694148\n",
       "0          CV  0.906449       NaN       NaN"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data type of the model\n",
    "frank_results_df_2 = pd.concat([results_df_2, grid_df_2], axis=0)\n",
    "frank_results_df_2 = frank_results_df_2[['Recall', \n",
    "                                     'ROC_AUC', \n",
    "                                       'Accuracy']]\n",
    "frank_results_df_2['Result_Type'] = ['Test'] * len(results_df) + ['CV'] * len(grid_df)\n",
    "frank_results_df_2 = frank_results_df_2[['Result_Type', 'Recall', 'ROC_AUC', 'Accuracy']]\n",
    "frank_results_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for XGBoost model\n",
    "\n",
    "name = 'Under_Sample_1:1_threshold_20'\n",
    "X_train_name = f\"{data_pth}{name}_X_train.parquet\"\n",
    "y_train_name = f\"{data_pth}{name}_y_train.parquet\"\n",
    "X_test_name = f\"{data_pth}{name}_X_test.parquet\"\n",
    "y_test_name = f\"{data_pth}{name}_y_test.parquet\"\n",
    "\n",
    "# Train and test sets.\n",
    "X_train = pd.read_parquet(X_train_name)\n",
    "y_train = pd.read_parquet(y_train_name)\n",
    "X_test = pd.read_parquet(X_test_name)\n",
    "y_test = pd.read_parquet(y_test_name)\n",
    "\n",
    "# Drop index column\n",
    "X_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "y_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "X_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "y_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "\n",
    "# Flatten response sets\n",
    "y_train = y_train.to_numpy().ravel()\n",
    "y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "# Standardize predictor sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of hyperparameters:  {'learning_rate': 0.001, 'max_depth': 3, 'subsample': 1}\n",
      "Best score:  0.8345360549838865\n"
     ]
    }
   ],
   "source": [
    "# Perform GridSearch\n",
    "\n",
    "# Define cross-validation folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=212)\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.5, 0.7, 1],\n",
    "}\n",
    "\n",
    "# Define GridSearch\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='recall',\n",
    "    n_jobs=8,\n",
    "    return_train_score=True)\n",
    "\n",
    "# Grid search training and results\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.7073852000260137\n"
     ]
    }
   ],
   "source": [
    "# Final XGBoost model\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model to make predictions on the testing data\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_probs = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Save XG Model\n",
    "pickle.dump(best_xgb_model, open(save_pth + 'xgb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost results\n",
    "xgb_results = {\n",
    "    \"Recall\": recall_score(y_test, predictions),\n",
    "    \"ROC_AUC\": roc_auc,\n",
    "    \"Accuracy\": accuracy_score(y_test, predictions)\n",
    "}\n",
    "xgb_results_df = pd.DataFrame(xgb_results, index=[0])\n",
    "xgb_results_df.to_parquet(f\"{save_pth}xgb_results.parquet\", index=False)\n",
    "\n",
    "# Read in XGBoost results\n",
    "xgb_results_df = pd.read_parquet(f\"{save_pth}xgb_results.parquet\")\n",
    "\n",
    "# Save grid search results\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_results.to_parquet(f\"{save_pth}grid_results_xgb.parquet\", index=False)\n",
    "\n",
    "# Read in grid search results\n",
    "grid_results_df = pd.read_parquet(f\"{save_pth}grid_results_xgb.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CV</td>\n",
       "      <td>0.834536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Result_Type    Recall\n",
       "20          CV  0.834536"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for best rank_test_score from grid_results_df\n",
    "best_cv_xgb = grid_results_df[grid_results_df['rank_test_score'] == 1]\n",
    "best_cv_xgb = best_cv_xgb['mean_test_score']\n",
    "# Convert to DataFrame\n",
    "best_cv_xgb = pd.DataFrame(best_cv_xgb)\n",
    "best_cv_xgb['Result_Type'] = 'CV'\n",
    "best_cv_xgb = best_cv_xgb[['Result_Type', 'mean_test_score']]\n",
    "# Change column name\n",
    "best_cv_xgb.columns = ['Result_Type', 'Recall']\n",
    "best_cv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CV</td>\n",
       "      <td>0.834536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Result_Type    Recall   ROC_AUC  Accuracy\n",
       "0         Test  0.705783  0.707385  0.662288\n",
       "20          CV  0.834536       NaN       NaN"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare results for table\n",
    "xgb_results_df['Result_Type'] = 'Test'\n",
    "xgb_results_df = xgb_results_df[['Result_Type', 'Recall', 'ROC_AUC', 'Accuracy']]\n",
    "\n",
    "# Combine results\n",
    "final_xgb_results_df = pd.concat([xgb_results_df, best_cv_xgb], axis=0)\n",
    "final_xgb_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_bayessearch(lazy_dict, unq_names, param_grid, save_pth, test_name, threads=None):\n",
    "    \"\"\"\n",
    "    MLP Bayesian Search using 5-fold Cross Validation. Saves best model and results.\n",
    "    ---\n",
    "    Args:\n",
    "        lazy_dict: Dictionary with names and LazyFrames of train and test sets.\n",
    "        unq_names: List of unique names of parent datasets.\n",
    "        param_grid: Dictionary with parameters for MLPClassifier.\n",
    "        save_pth: String of the path to save the best model.\n",
    "        test_name: String of the test performed. PARAMETER BEING TESTED\n",
    "        threads: Integer of CPU threads for cross-validation (optional).\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initializing\n",
    "    if threads is None:\n",
    "        threads = os.cpu_count() - 4\n",
    "        print(f\"Using {threads} CPU threads!\")\n",
    "\n",
    "    # Log for debugging\n",
    "    logging.basicConfig(\n",
    "        filename=f\"./log/MLP_{test_name}.log\",\n",
    "        filemode='w', \n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    for name in unq_names:\n",
    "        best_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"ROC_AUC\": [], \n",
    "            \"Accuracy\": [],\n",
    "            \"Fit_Time\": []\n",
    "        }\n",
    "        \n",
    "        param_results = {\n",
    "            \"Dataset_Name\": [],\n",
    "            \"Grid_Variable\": [],\n",
    "            \"Parameters\": [],\n",
    "            \"Recall\": [], \n",
    "            \"Fit_Time\": []\n",
    "        }\n",
    "        \n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        X_test_name = f\"{name}_X_test\"\n",
    "        y_test_name = f\"{name}_y_test\"\n",
    "\n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_pandas()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_pandas()\n",
    "\n",
    "        X_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        X_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "\n",
    "        y_train = y_train.to_numpy().ravel()\n",
    "        y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=212)\n",
    "        mlp_model = mlp()\n",
    "\n",
    "        grid_search = BayesSearchCV(\n",
    "            mlp_model,  # MLP model\n",
    "            search_spaces=param_grid,  # Parameter dictionary\n",
    "            cv=cv,  # Cross-validation\n",
    "            scoring='recall',  # Best is by recall\n",
    "            n_iter=10,  # Number of iterations\n",
    "            n_jobs=threads,\n",
    "            verbose=3,\n",
    "            return_train_score=True  # For making CV table\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Processing dataset: {name}\")\n",
    "        print(f\"Training on {name}...\", flush=True)\n",
    "\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        print(f\"GridSearch completed\", flush=True)\n",
    "        logging.info(f\"GridSearch for {test_name} completed.\")\n",
    "\n",
    "        for i in range(len(grid_search.cv_results_['params'])):\n",
    "            param_combination = grid_search.cv_results_['params'][i]\n",
    "            recall = grid_search.cv_results_['mean_test_score'][i]\n",
    "            fit_time = grid_search.cv_results_['mean_fit_time'][i]\n",
    "            param_results[\"Dataset_Name\"].append(name)\n",
    "            param_results[\"Grid_Variable\"].append(test_name)\n",
    "            param_results[\"Parameters\"].append(param_combination)\n",
    "            param_results[\"Recall\"].append(recall)\n",
    "            param_results[\"Fit_Time\"].append(fit_time)\n",
    "        \n",
    "        param_results_df = pd.DataFrame(param_results)\n",
    "        param_results_df = param_results_df.sort_values(by=\"Recall\", ascending=False)\n",
    "\n",
    "        best_fit_time = param_results_df.iloc[0][\"Fit_Time\"]\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        test_recall = recall_score(y_test, y_pred_test)\n",
    "        test_roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        with open(f\"{save_pth}best_model{test_name}-{name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(best_model, file)\n",
    "\n",
    "        logging.info(f\"Model saved to {save_pth}\")\n",
    "\n",
    "        best_results[\"Dataset_Name\"].append(name)\n",
    "        best_results[\"Grid_Variable\"].append(test_name)\n",
    "        best_results[\"Parameters\"].append(grid_search.best_params_)\n",
    "        best_results[\"Recall\"].append(test_recall)\n",
    "        best_results[\"ROC_AUC\"].append(test_roc_auc)\n",
    "        best_results[\"Accuracy\"].append(test_accuracy)\n",
    "        best_results[\"Fit_Time\"].append(best_fit_time)\n",
    "\n",
    "        best_results_df = pd.DataFrame(best_results)\n",
    "        best_results_df.to_parquet(f\"{save_pth}test_results{test_name}-{name}.parquet\", index=False)\n",
    "        param_results_df.to_parquet(f\"{save_pth}grid_results{test_name}-{name}.parquet\", index=False)\n",
    "\n",
    "        print(f\"{test_name} GridSearch completed!\", flush=True)\n",
    "        logging.info(f\"{test_name} GridSearch completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets are in MLP_Dataset.\n",
    "# Save results and best model to MLP_Results.\n",
    "data_pth = \"../../../Data/GoogleDrive/MLP_Dataset/\"\n",
    "save_pth = \"../../../Data/GoogleDrive/MLP_Results/\"\n",
    "\n",
    "# Read in Parquet files in path and add to a LazyFrame dictionary.\n",
    "pq_jar = parquet_to_dict(data_pth) # all lazy\n",
    "\n",
    "# Record the unique dataset names (drop X_train, etc.).\n",
    "unq_names = unq_df_names(pq_jar)\n",
    "\n",
    "test = '_bayes_params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': Integer(10, 100),\n",
    "    'activation': Categorical(categories=['relu', 'tanh', 'logistic']),\n",
    "    'alpha': Real(0.0, 0.001),\n",
    "    'batch_size': Categorical(categories=['auto', 100, 1000]),\n",
    "    'learning_rate': Categorical(categories=['constant', 'adaptive']),\n",
    "    'learning_rate_init': Real(0.001, 0.01, prior='log-uniform'),\n",
    "    'max_iter': Integer(10, 50),\n",
    "    'n_iter_no_change': Integer(1, 5),\n",
    "    'solver': Categorical(categories=['adam', 'sgd'])\n",
    "}\n",
    "\n",
    "param_grid = [param_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 CPU threads!\n",
      "Training on Under_Sample_1:1_threshold_20...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation=tanh, alpha=0.0005780833964463797, batch_size=auto, hidden_layer_sizes=39, learning_rate=adaptive, learning_rate_init=0.0013357968991107108, max_iter=11, n_iter_no_change=3, solver=sgd;, score=(train=0.790, test=0.794) total time=   7.2s\n",
      "[CV 4/5] END activation=tanh, alpha=0.0005780833964463797, batch_size=auto, hidden_layer_sizes=39, learning_rate=adaptive, learning_rate_init=0.0013357968991107108, max_iter=11, n_iter_no_change=3, solver=sgd;, score=(train=0.791, test=0.788) total time=   7.2s\n",
      "[CV 3/5] END activation=tanh, alpha=0.0005780833964463797, batch_size=auto, hidden_layer_sizes=39, learning_rate=adaptive, learning_rate_init=0.0013357968991107108, max_iter=11, n_iter_no_change=3, solver=sgd;, score=(train=0.790, test=0.788) total time=   7.3s\n",
      "[CV 5/5] END activation=tanh, alpha=0.0005780833964463797, batch_size=auto, hidden_layer_sizes=39, learning_rate=adaptive, learning_rate_init=0.0013357968991107108, max_iter=11, n_iter_no_change=3, solver=sgd;, score=(train=0.792, test=0.790) total time=   7.3s\n",
      "[CV 1/5] END activation=tanh, alpha=0.0005780833964463797, batch_size=auto, hidden_layer_sizes=39, learning_rate=adaptive, learning_rate_init=0.0013357968991107108, max_iter=11, n_iter_no_change=3, solver=sgd;, score=(train=0.788, test=0.785) total time=   7.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 3/5] END activation=logistic, alpha=0.000401690440728251, batch_size=auto, hidden_layer_sizes=35, learning_rate=constant, learning_rate_init=0.00874591395954147, max_iter=26, n_iter_no_change=3, solver=adam;, score=(train=0.769, test=0.760) total time=  14.5s\n",
      "[CV 2/5] END activation=logistic, alpha=0.000401690440728251, batch_size=auto, hidden_layer_sizes=35, learning_rate=constant, learning_rate_init=0.00874591395954147, max_iter=26, n_iter_no_change=3, solver=adam;, score=(train=0.836, test=0.831) total time=  15.4s\n",
      "[CV 1/5] END activation=logistic, alpha=0.000401690440728251, batch_size=auto, hidden_layer_sizes=35, learning_rate=constant, learning_rate_init=0.00874591395954147, max_iter=26, n_iter_no_change=3, solver=adam;, score=(train=0.815, test=0.805) total time=  17.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=logistic, alpha=0.000401690440728251, batch_size=auto, hidden_layer_sizes=35, learning_rate=constant, learning_rate_init=0.00874591395954147, max_iter=26, n_iter_no_change=3, solver=adam;, score=(train=0.819, test=0.811) total time=  18.4s\n",
      "[CV 4/5] END activation=logistic, alpha=0.000401690440728251, batch_size=auto, hidden_layer_sizes=35, learning_rate=constant, learning_rate_init=0.00874591395954147, max_iter=26, n_iter_no_change=3, solver=adam;, score=(train=0.832, test=0.821) total time=  18.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation=relu, alpha=0.0005235656030788829, batch_size=100, hidden_layer_sizes=94, learning_rate=adaptive, learning_rate_init=0.0031963560553544723, max_iter=35, n_iter_no_change=5, solver=adam;, score=(train=0.847, test=0.837) total time=  48.9s\n",
      "[CV 1/5] END activation=relu, alpha=0.0005235656030788829, batch_size=100, hidden_layer_sizes=94, learning_rate=adaptive, learning_rate_init=0.0031963560553544723, max_iter=35, n_iter_no_change=5, solver=adam;, score=(train=0.847, test=0.831) total time=  49.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=relu, alpha=0.0005235656030788829, batch_size=100, hidden_layer_sizes=94, learning_rate=adaptive, learning_rate_init=0.0031963560553544723, max_iter=35, n_iter_no_change=5, solver=adam;, score=(train=0.848, test=0.835) total time=  50.0s\n",
      "[CV 3/5] END activation=relu, alpha=0.0005235656030788829, batch_size=100, hidden_layer_sizes=94, learning_rate=adaptive, learning_rate_init=0.0031963560553544723, max_iter=35, n_iter_no_change=5, solver=adam;, score=(train=0.827, test=0.813) total time=  50.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation=relu, alpha=0.0005235656030788829, batch_size=100, hidden_layer_sizes=94, learning_rate=adaptive, learning_rate_init=0.0031963560553544723, max_iter=35, n_iter_no_change=5, solver=adam;, score=(train=0.823, test=0.808) total time=  52.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation=logistic, alpha=0.0007155671543690813, batch_size=1000, hidden_layer_sizes=63, learning_rate=adaptive, learning_rate_init=0.0032852505449157427, max_iter=34, n_iter_no_change=1, solver=adam;, score=(train=0.840, test=0.823) total time=  29.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation=logistic, alpha=0.0007155671543690813, batch_size=1000, hidden_layer_sizes=63, learning_rate=adaptive, learning_rate_init=0.0032852505449157427, max_iter=34, n_iter_no_change=1, solver=adam;, score=(train=0.846, test=0.832) total time=  30.6s\n",
      "[CV 2/5] END activation=logistic, alpha=0.0007155671543690813, batch_size=1000, hidden_layer_sizes=63, learning_rate=adaptive, learning_rate_init=0.0032852505449157427, max_iter=34, n_iter_no_change=1, solver=adam;, score=(train=0.825, test=0.811) total time=  30.6s\n",
      "[CV 4/5] END activation=logistic, alpha=0.0007155671543690813, batch_size=1000, hidden_layer_sizes=63, learning_rate=adaptive, learning_rate_init=0.0032852505449157427, max_iter=34, n_iter_no_change=1, solver=adam;, score=(train=0.832, test=0.814) total time=  30.6s\n",
      "[CV 5/5] END activation=logistic, alpha=0.0007155671543690813, batch_size=1000, hidden_layer_sizes=63, learning_rate=adaptive, learning_rate_init=0.0032852505449157427, max_iter=34, n_iter_no_change=1, solver=adam;, score=(train=0.848, test=0.832) total time=  30.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation=tanh, alpha=0.0008977514539113366, batch_size=100, hidden_layer_sizes=68, learning_rate=constant, learning_rate_init=0.005717502700810796, max_iter=38, n_iter_no_change=2, solver=sgd;, score=(train=0.835, test=0.820) total time=  39.8s\n",
      "[CV 2/5] END activation=tanh, alpha=0.0008977514539113366, batch_size=100, hidden_layer_sizes=68, learning_rate=constant, learning_rate_init=0.005717502700810796, max_iter=38, n_iter_no_change=2, solver=sgd;, score=(train=0.827, test=0.812) total time=  40.1s\n",
      "[CV 1/5] END activation=tanh, alpha=0.0008977514539113366, batch_size=100, hidden_layer_sizes=68, learning_rate=constant, learning_rate_init=0.005717502700810796, max_iter=38, n_iter_no_change=2, solver=sgd;, score=(train=0.836, test=0.822) total time=  40.3s\n",
      "[CV 3/5] END activation=tanh, alpha=0.0008977514539113366, batch_size=100, hidden_layer_sizes=68, learning_rate=constant, learning_rate_init=0.005717502700810796, max_iter=38, n_iter_no_change=2, solver=sgd;, score=(train=0.827, test=0.811) total time=  40.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=tanh, alpha=0.0008977514539113366, batch_size=100, hidden_layer_sizes=68, learning_rate=constant, learning_rate_init=0.005717502700810796, max_iter=38, n_iter_no_change=2, solver=sgd;, score=(train=0.838, test=0.822) total time=  42.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=logistic, alpha=0.0008749514676217866, batch_size=auto, hidden_layer_sizes=71, learning_rate=adaptive, learning_rate_init=0.0017500477004218267, max_iter=13, n_iter_no_change=4, solver=sgd;, score=(train=0.778, test=0.779) total time=  14.0s\n",
      "[CV 3/5] END activation=logistic, alpha=0.0008749514676217866, batch_size=auto, hidden_layer_sizes=71, learning_rate=adaptive, learning_rate_init=0.0017500477004218267, max_iter=13, n_iter_no_change=4, solver=sgd;, score=(train=0.791, test=0.790) total time=  13.9s\n",
      "[CV 4/5] END activation=logistic, alpha=0.0008749514676217866, batch_size=auto, hidden_layer_sizes=71, learning_rate=adaptive, learning_rate_init=0.0017500477004218267, max_iter=13, n_iter_no_change=4, solver=sgd;, score=(train=0.789, test=0.786) total time=  14.2s\n",
      "[CV 2/5] END activation=logistic, alpha=0.0008749514676217866, batch_size=auto, hidden_layer_sizes=71, learning_rate=adaptive, learning_rate_init=0.0017500477004218267, max_iter=13, n_iter_no_change=4, solver=sgd;, score=(train=0.786, test=0.790) total time=  14.0s\n",
      "[CV 1/5] END activation=logistic, alpha=0.0008749514676217866, batch_size=auto, hidden_layer_sizes=71, learning_rate=adaptive, learning_rate_init=0.0017500477004218267, max_iter=13, n_iter_no_change=4, solver=sgd;, score=(train=0.787, test=0.786) total time=  14.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation=relu, alpha=0.000617392736357786, batch_size=1000, hidden_layer_sizes=46, learning_rate=constant, learning_rate_init=0.0064129598516722485, max_iter=10, n_iter_no_change=3, solver=adam;, score=(train=0.814, test=0.806) total time=   6.1s\n",
      "[CV 5/5] END activation=relu, alpha=0.000617392736357786, batch_size=1000, hidden_layer_sizes=46, learning_rate=constant, learning_rate_init=0.0064129598516722485, max_iter=10, n_iter_no_change=3, solver=adam;, score=(train=0.818, test=0.813) total time=   6.1s\n",
      "[CV 2/5] END activation=relu, alpha=0.000617392736357786, batch_size=1000, hidden_layer_sizes=46, learning_rate=constant, learning_rate_init=0.0064129598516722485, max_iter=10, n_iter_no_change=3, solver=adam;, score=(train=0.803, test=0.799) total time=   6.1s\n",
      "[CV 3/5] END activation=relu, alpha=0.000617392736357786, batch_size=1000, hidden_layer_sizes=46, learning_rate=constant, learning_rate_init=0.0064129598516722485, max_iter=10, n_iter_no_change=3, solver=adam;, score=(train=0.805, test=0.801) total time=   6.1s\n",
      "[CV 4/5] END activation=relu, alpha=0.000617392736357786, batch_size=1000, hidden_layer_sizes=46, learning_rate=constant, learning_rate_init=0.0064129598516722485, max_iter=10, n_iter_no_change=3, solver=adam;, score=(train=0.798, test=0.791) total time=   6.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=logistic, alpha=0.0001278804199518171, batch_size=100, hidden_layer_sizes=76, learning_rate=adaptive, learning_rate_init=0.001252331317985851, max_iter=42, n_iter_no_change=4, solver=sgd;, score=(train=0.788, test=0.787) total time=  53.1s\n",
      "[CV 2/5] END activation=logistic, alpha=0.0001278804199518171, batch_size=100, hidden_layer_sizes=76, learning_rate=adaptive, learning_rate_init=0.001252331317985851, max_iter=42, n_iter_no_change=4, solver=sgd;, score=(train=0.785, test=0.788) total time=  53.2s\n",
      "[CV 1/5] END activation=logistic, alpha=0.0001278804199518171, batch_size=100, hidden_layer_sizes=76, learning_rate=adaptive, learning_rate_init=0.001252331317985851, max_iter=42, n_iter_no_change=4, solver=sgd;, score=(train=0.789, test=0.789) total time=  53.4s\n",
      "[CV 3/5] END activation=logistic, alpha=0.0001278804199518171, batch_size=100, hidden_layer_sizes=76, learning_rate=adaptive, learning_rate_init=0.001252331317985851, max_iter=42, n_iter_no_change=4, solver=sgd;, score=(train=0.796, test=0.794) total time=  53.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation=logistic, alpha=0.0001278804199518171, batch_size=100, hidden_layer_sizes=76, learning_rate=adaptive, learning_rate_init=0.001252331317985851, max_iter=42, n_iter_no_change=4, solver=sgd;, score=(train=0.809, test=0.806) total time=  54.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation=logistic, alpha=0.0009194432239119835, batch_size=100, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.001468857980439449, max_iter=41, n_iter_no_change=3, solver=sgd;, score=(train=0.810, test=0.812) total time=  54.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=logistic, alpha=0.0009194432239119835, batch_size=100, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.001468857980439449, max_iter=41, n_iter_no_change=3, solver=sgd;, score=(train=0.802, test=0.803) total time=  55.4s\n",
      "[CV 1/5] END activation=logistic, alpha=0.0009194432239119835, batch_size=100, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.001468857980439449, max_iter=41, n_iter_no_change=3, solver=sgd;, score=(train=0.800, test=0.799) total time=  55.6s\n",
      "[CV 3/5] END activation=logistic, alpha=0.0009194432239119835, batch_size=100, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.001468857980439449, max_iter=41, n_iter_no_change=3, solver=sgd;, score=(train=0.783, test=0.780) total time=  55.9s\n",
      "[CV 4/5] END activation=logistic, alpha=0.0009194432239119835, batch_size=100, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.001468857980439449, max_iter=41, n_iter_no_change=3, solver=sgd;, score=(train=0.805, test=0.802) total time=  55.9s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation=tanh, alpha=0.00017133891081233313, batch_size=1000, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.0029066648194298653, max_iter=48, n_iter_no_change=2, solver=sgd;, score=(train=0.795, test=0.793) total time=  43.6s\n",
      "[CV 2/5] END activation=tanh, alpha=0.00017133891081233313, batch_size=1000, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.0029066648194298653, max_iter=48, n_iter_no_change=2, solver=sgd;, score=(train=0.795, test=0.797) total time=  43.5s\n",
      "[CV 5/5] END activation=tanh, alpha=0.00017133891081233313, batch_size=1000, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.0029066648194298653, max_iter=48, n_iter_no_change=2, solver=sgd;, score=(train=0.798, test=0.795) total time=  43.4s\n",
      "[CV 3/5] END activation=tanh, alpha=0.00017133891081233313, batch_size=1000, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.0029066648194298653, max_iter=48, n_iter_no_change=2, solver=sgd;, score=(train=0.797, test=0.794) total time=  43.5s\n",
      "[CV 4/5] END activation=tanh, alpha=0.00017133891081233313, batch_size=1000, hidden_layer_sizes=79, learning_rate=adaptive, learning_rate_init=0.0029066648194298653, max_iter=48, n_iter_no_change=2, solver=sgd;, score=(train=0.796, test=0.791) total time=  43.6s\n",
      "GridSearch completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert 'auto' with type str: tried to convert to int64\", 'Conversion failed for column Parameters with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[155], line 126\u001b[0m, in \u001b[0;36mmlp_bayessearch\u001b[0;34m(lazy_dict, unq_names, param_grid, save_pth, test_name, threads)\u001b[0m\n\u001b[1;32m    124\u001b[0m best_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(best_results)\n\u001b[1;32m    125\u001b[0m best_results_df\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_pth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtest_results\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 126\u001b[0m param_results_df\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_pth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mgrid_results\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GridSearch completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GridSearch completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:2970\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2883\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2884\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2966\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2971\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2972\u001b[0m     path,\n\u001b[1;32m   2973\u001b[0m     engine,\n\u001b[1;32m   2974\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   2975\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   2976\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[1;32m   2977\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:483\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    481\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 483\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    484\u001b[0m     df,\n\u001b[1;32m    485\u001b[0m     path_or_buf,\n\u001b[1;32m    486\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    487\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    488\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[1;32m    489\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    490\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 189\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    192\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/table.pxi:3869\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:613\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    609\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    610\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 613\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [convert_column(c, f)\n\u001b[1;32m    614\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:613\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    609\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    610\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 613\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [convert_column(c, f)\n\u001b[1;32m    614\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:600\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    596\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    597\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    598\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    604\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:594\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    591\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(col, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtype_, from_pandas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, safe\u001b[38;5;241m=\u001b[39msafe)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    596\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    597\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    598\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/array.pxi:340\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/array.pxi:86\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: (\"Could not convert 'auto' with type str: tried to convert to int64\", 'Conversion failed for column Parameters with type object')"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlp_bayessearch(pq_jar, unq_names, param_grid, save_pth, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "data_pth = \"../../../Data/GoogleDrive/MLP_Dataset/\"\n",
    "save_pth = \"../../../Data/GoogleDrive/MLP_Results/\"\n",
    "name = 'bayes_params-Under_Sample_1:1_threshold_20'\n",
    "\n",
    "# Best model by Recall\n",
    "with open(f\"{save_pth}best_model_{name}.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Test set prediction results\n",
    "results_df = pd.read_parquet(f\"{save_pth}test_results_{name}.parquet\")\n",
    "\n",
    "# Cross Validation results\n",
    "# grid_df = pd.read_parquet(f\"{save_pth}grid_results_{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Grid_Variable</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_bayes_params</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.000523565603...</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "      <td>49.996617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Dataset_Name  Grid_Variable  \\\n",
       "0  Under_Sample_1:1_threshold_20  _bayes_params   \n",
       "\n",
       "                                          Parameters    Recall   ROC_AUC  \\\n",
       "0  {'activation': 'relu', 'alpha': 0.000523565603...  0.770816  0.813239   \n",
       "\n",
       "   Accuracy   Fit_Time  \n",
       "0  0.722086  49.996617  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Result_Type    Recall   ROC_AUC  Accuracy\n",
       "0        Test  0.770816  0.813239  0.722086"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data type of the model\n",
    "bayes_results_df = pd.concat([results_df], axis=0)\n",
    "bayes_results_df = bayes_results_df[['Recall', \n",
    "                                     'ROC_AUC', \n",
    "                                       'Accuracy']]\n",
    "bayes_results_df['Result_Type'] = ['Test'] * len(results_df)\n",
    "bayes_results_df = bayes_results_df[['Result_Type', 'Recall', 'ROC_AUC', 'Accuracy']]\n",
    "bayes_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.694148</td>\n",
       "      <td>Frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "      <td>Bayes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Result_Type    Recall   ROC_AUC  Accuracy         Model\n",
       "0        Test  0.815167  0.815958  0.694148  Frankenstein\n",
       "0        Test  0.705783  0.707385  0.662288       XGBoost\n",
       "0        Test  0.770816  0.813239  0.722086         Bayes"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate results (Frankenstein, XGB, Bayes)\n",
    "overall_results = pd.concat([frank_results_df_2, final_xgb_results_df, bayes_results_df], axis=0)\n",
    "# add column for model name (Frankenstein, XGB, Bayes)\n",
    "overall_results['Model'] = ['Frankenstein'] * len(frank_results_df_2) + ['XGBoost'] * len(final_xgb_results_df) + ['Bayes'] * len(bayes_results_df)\n",
    "# filter out CV results\n",
    "overall_results = overall_results[overall_results['Result_Type'] == 'Test']\n",
    "overall_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
