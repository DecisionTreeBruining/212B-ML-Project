{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1238c4de-bef0-427c-a31c-17418e99f09f",
   "metadata": {},
   "source": [
    "# Baseline Multi-Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e073e2-05b2-4fc6-9574-de83ef7c7f35",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074514d8-c5dc-46e5-ac94-9efaf39acdad",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Notebook Setup](#Notebook-Setup)\n",
    "- [Read in Parquet](#Read-in-Parquet)\n",
    "- [MLP Baseline Model Loop](#MLP-Baseline-Model-Loop)\n",
    "- [MLP Baseline Parallelization](#MLP-Baseline-Parallelization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53820c5-2013-433a-b156-864143ef2b64",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Significant functions from [assignment_3_tools.py](./assignment_3_tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cb0916-cef7-4ebd-857c-814f6db863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle #for saveing and loading trained models\n",
    "import numpy as np # for vector / matrix operations\n",
    "import pandas as pd # for data manipulation\n",
    "import seaborn as sns # For plots\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, make_scorer, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "from assignment_3_tools import parquet_to_dict\n",
    "\n",
    "data_pth = \"../../Data/GoogleDrive/Encoded_Data/\"\n",
    "save_pth = \"../../Data/GoogleDrive/Baseline/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cedacb-a494-4f84-8bad-42e9e86c411e",
   "metadata": {},
   "source": [
    "## Read in Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a079661f-0efa-4a34-8b58-edf66898417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy read encoded data.\n",
    "pq_jar = parquet_to_dict(data_pth)\n",
    "\n",
    "# Unique Datasets.\n",
    "def encode_dataset(lazy_dict):\n",
    "    all_names = list()\n",
    "    for i, key in enumerate(lazy_dict):\n",
    "        if key[-6:] == \"_train\":\n",
    "            all_names.append(key[:-8])\n",
    "        elif key[-5:] == \"_test\":\n",
    "            all_names.append(key[:-7])\n",
    "        else:\n",
    "            pass\n",
    "    unq_names = set(all_names)\n",
    "    return unq_names\n",
    "\n",
    "unq_names = encode_dataset(pq_jar)\n",
    "\n",
    "# Return Corresponding Test Set.\n",
    "def corr_testset(unq_name):\n",
    "    threshold = unq_name[-2:]\n",
    "    if threshold.isnumeric():\n",
    "        X_test_name = f\"df_heart_drop_{threshold}_imp_X_test\"\n",
    "        y_test_name = f\"df_heart_drop_{threshold}_imp_y_test\"\n",
    "    else:\n",
    "        X_test_name = f\"{unq_name}_X_test\"\n",
    "        y_test_name = f\"{unq_name}_y_test\"\n",
    "    return X_test_name, y_test_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e2525-b076-4432-8f3e-0303ffab4f27",
   "metadata": {},
   "source": [
    "## MLP Baseline Model Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b369f3-cf02-449e-83d4-fd022340e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_baseline(lazy_dict, save_pth):\n",
    "    baseline_results = dict()\n",
    "    # Initialize MLP parameters\n",
    "    params = {'activation':'tanh', 'solver':'adam', 'valid_frac':0.2,\n",
    "      'alpha':0.001, 'learn_rate_init':0.0001,\n",
    "      'max_iter':1000, 'n_iter_no_change':200,\n",
    "      'rand_state': 3}\n",
    "    \n",
    "    # MLP for each Dataframe\n",
    "    for name in unq_names:\n",
    "        # pl.LazyFrame Names\n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        (X_test_name, y_test_name) = corr_testset(name)\n",
    "        \n",
    "        # Collect pl.LazyFrame and convert to pd.DataFrame\n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_numpy().ravel()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_numpy().ravel()\n",
    "        \n",
    "        # Remove index column\n",
    "        if \"__index_level_0__\" in X_train.columns:\n",
    "            X_train = X_train.drop(columns=['__index_level_0__'])\n",
    "        if \"__index_level_0__\" in X_test.columns:\n",
    "            X_test = X_test.drop(columns=['__index_level_0__'])\n",
    "            \n",
    "        # Standardize X_train and X_test by the standardization scalar of X_train\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        print(f\"Training MLP on {name}...\")\n",
    "        \n",
    "        # Initialize mlp model\n",
    "        mlp_model = mlp(\n",
    "            hidden_layer_sizes=([8,4,8]),\n",
    "            validation_fraction = params['valid_frac'],\n",
    "            activation = params['activation'],\n",
    "            solver = params['solver'],\n",
    "            alpha = params['alpha'],\n",
    "            learning_rate = \"adaptive\",\n",
    "            learning_rate_init = params['learn_rate_init'],\n",
    "            batch_size = \"auto\",\n",
    "            max_iter = params['max_iter'],\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = params['n_iter_no_change'],\n",
    "            verbose= False,\n",
    "            random_state = params['rand_state'])\n",
    "        \n",
    "        # Train the model\n",
    "        mlp_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Save the Trained model\n",
    "        with open(f\"{save_pth}{name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(mlp_model, file)\n",
    "            \n",
    "        # Cross-Validation and Evaluation\n",
    "        scoring = {'recall': make_scorer(recall_score)}\n",
    "        scores = cross_validate(mlp_model, X_train_scaled, y_train, cv=5, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        # Store results in the dictionary\n",
    "        baseline_results['dataset_name'].append(name)\n",
    "        baseline_results['train_recall'].append(scores['train_recall'].mean())\n",
    "        baseline_results['test_recall'].append(scores['test_recall'].mean())\n",
    "        baseline_results['fit_time'].append(scores['fit_time'].mean())\n",
    "        print(f\"Completed Training MLP on {name}!\")\n",
    "    df_baseline_results = pd.DataFrame(baseline_results)\n",
    "    df_baseline_results.to_parquet(f\"{save_pth}baseline_results.pkl\")\n",
    "    return df_baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c122f60-86d6-4fcd-aabe-4af7a113f4c3",
   "metadata": {},
   "source": [
    "## MLP Baseline Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c235b298-ed3e-43a6-be61-41abdd99d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/203C/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/anaconda3/envs/203C/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/anaconda3/envs/203C/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/anaconda3/envs/203C/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP on Under_Sample_3:1_threshold_03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def mlp_baseline_parallel(lazy_dict, save_pth, threads=None):\n",
    "    if threads is None:\n",
    "        threads = os.cpu_count() - 2\n",
    "    baseline_results = dict()\n",
    "    params = {'activation':'tanh', 'solver':'adam', 'valid_frac':0.2,\n",
    "      'alpha':0.001, 'learn_rate_init':0.0001,\n",
    "      'max_iter':1000, 'n_iter_no_change':200,\n",
    "      'rand_state': 3}\n",
    "\n",
    "    # Helper function to process each dataset in parallel\n",
    "    def process_dataset(name, lazy_dict):\n",
    "        print(f\"Training MLP on {name}...\", flush=True)\n",
    "\n",
    "        # pl.LazyFrame Names\n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        (X_test_name, y_test_name) = corr_testset(name)\n",
    "\n",
    "        # Collect pl.LazyFrame and convert to NumPy arrays (reset index for X)\n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_numpy().ravel()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_numpy().ravel()\n",
    "\n",
    "        # Remove index column\n",
    "        if \"__index_level_0__\" in X_train.columns:\n",
    "            X_train = X_train.drop(columns=['__index_level_0__'])\n",
    "        if \"__index_level_0__\" in X_test.columns:\n",
    "            X_test = X_test.drop(columns=['__index_level_0__'])\n",
    "\n",
    "        # Standardize X_train and X_test\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Initialize mlp model\n",
    "        mlp_model = mlp(\n",
    "            hidden_layer_sizes=([8, 4, 8]),\n",
    "            validation_fraction = params['valid_frac'],\n",
    "            activation = params['activation'],\n",
    "            solver = params['solver'],\n",
    "            alpha = params['alpha'],\n",
    "            learning_rate = \"adaptive\",\n",
    "            learning_rate_init = params['learn_rate_init'],\n",
    "            batch_size = \"auto\",\n",
    "            max_iter = params['max_iter'],\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = params['n_iter_no_change'],\n",
    "            verbose= False,\n",
    "            random_state = params['rand_state'])\n",
    "\n",
    "        # Train the model\n",
    "        mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Save the Trained model\n",
    "        with open(f\"{save_pth}{name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(mlp_model, file)\n",
    "\n",
    "        # Cross-Validation and Evaluation\n",
    "        scoring = {'recall': make_scorer(recall_score)}\n",
    "        scores = cross_validate(mlp_model, X_train_scaled, y_train, cv=5, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        print(f\"Completed MLP on {name}!\", flush=True)  # Added for progress tracking\n",
    "\n",
    "        return name, scores\n",
    "\n",
    "    # Use Joblib for parallel processing\n",
    "    with Parallel(n_jobs=threads) as parallel:\n",
    "        results = parallel(delayed(process_dataset)(name, lazy_dict) for name in unq_names)\n",
    "\n",
    "    # Extract results from the parallel runs\n",
    "    for name, scores in results:\n",
    "        baseline_results['dataset_name'].append(name)\n",
    "        baseline_results['train_recall'].append(scores['train_recall'].mean())\n",
    "        baseline_results['test_recall'].append(scores['test_recall'].mean())\n",
    "        baseline_results['fit_time'].append(scores['fit_time'].mean())\n",
    "        \n",
    "    df_baseline_results = pd.DataFrame(baseline_results)\n",
    "    df_baseline_results.to_parquet(f\"{save_pth}baseline_results.pkl\")\n",
    "    \n",
    "    return df_baseline_results\n",
    "\n",
    "mlp_baseline_parallel(pq_jar, save_pth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "203C",
   "language": "python",
   "name": "203c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
