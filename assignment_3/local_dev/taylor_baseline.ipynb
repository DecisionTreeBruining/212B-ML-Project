{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1238c4de-bef0-427c-a31c-17418e99f09f",
   "metadata": {},
   "source": [
    "# Baseline Multi-Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e073e2-05b2-4fc6-9574-de83ef7c7f35",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074514d8-c5dc-46e5-ac94-9efaf39acdad",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Notebook Setup](#Notebook-Setup)\n",
    "- [Read in Parquet](#Read-in-Parquet)\n",
    "- [MLP Baseline Model Loop](#MLP-Baseline-Model-Loop)\n",
    "- [MLP Baseline Parallelization](#MLP-Baseline-Parallelization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53820c5-2013-433a-b156-864143ef2b64",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Significant functions from [assignment_3_tools.py](./assignment_3_tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cb0916-cef7-4ebd-857c-814f6db863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle #for saveing and loading trained models\n",
    "import numpy as np # for vector / matrix operations\n",
    "import pandas as pd # for data manipulation\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "from assignment_3_tools import parquet_to_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cedacb-a494-4f84-8bad-42e9e86c411e",
   "metadata": {},
   "source": [
    "## Unique Datasets and Corresponding Testsets\n",
    "\n",
    "In [preprocess notebook](./taylor_preprocess.ipynb), all of the null-threshold datasets were split into X_train, y_train, X_test, and y_test. The X_train, and y_train sets of each null-threshold datasets were balanced using random over/under sampling. Therefore when `parquet_to_dict()` is called, the dictionary will contain the X_train, y_train, X_test, y_test which correspond to one dataset. To resolve this, `unq_df_names()` and `corr_testset` record the dataset names and corresponding testsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe680fe-f9c3-4edf-b121-7048495cf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Datasets.\n",
    "def unq_df_names(lazy_dict):\n",
    "    \"\"\"\n",
    "    Creates a set of unique datasets from a LazyFrame dictionary.\n",
    "    ---\n",
    "    Args: \n",
    "        lazy_dict (dict): Contains LazyFrame names and corresponding LazyFrames.\n",
    "    Returns:\n",
    "        unq_names (set): Contains unique dataset names.\n",
    "    \"\"\"\n",
    "    all_names = list()\n",
    "    for key in lazy_dict:\n",
    "        if key[-6:] == \"_train\":\n",
    "            all_names.append(key[:-8]) # Remove _X_train and _y_train\n",
    "        elif key[-5:] == \"_test\":\n",
    "            all_names.append(key[:-7]) # Remove _X_test and _y_test\n",
    "        else:\n",
    "            pass\n",
    "    unq_names = set(all_names)\n",
    "    return unq_names\n",
    "\n",
    "# Return Corresponding Test Set.\n",
    "def corr_testset(unq_name):\n",
    "    \"\"\"\n",
    "    Return the names of testsets corresponding to a preprocessed trainset\n",
    "    ---\n",
    "    Args:\n",
    "        unq_name(set): Contains unique dataset names.\n",
    "    Returns:\n",
    "        X_test_name(str): Name of corresponding predictor testset.\n",
    "        y_test_name(str): Name of corresponding response testset.\n",
    "    \"\"\"\n",
    "    threshold = unq_name[-2:] # 2 possibilities: \"##\" or \"mp\"\n",
    "    if threshold.isnumeric():\n",
    "        # Use null-threshold datasets with no balancing operations.\n",
    "        X_test_name = f\"df_heart_drop_{threshold}_imp_X_test\"\n",
    "        y_test_name = f\"df_heart_drop_{threshold}_imp_y_test\"\n",
    "    else:\n",
    "        # Use null-threshold datasets with no balancing operations. \n",
    "        X_test_name = f\"{unq_name}_X_test\"\n",
    "        y_test_name = f\"{unq_name}_y_test\"\n",
    "    return X_test_name, y_test_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e2525-b076-4432-8f3e-0303ffab4f27",
   "metadata": {},
   "source": [
    "## MLP Baseline Model Loop\n",
    "\n",
    "This MLP model loops through all of the unique dataset names from `unq_df_names` and trains an MLP model on each unique dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b369f3-cf02-449e-83d4-fd022340e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_baseline(lazy_dict, unq_names, param_grid, save_pth, threads=None):\n",
    "    \"\"\"\n",
    "    Baseline MLP model using 5-fold Cross Validation.\n",
    "    ---\n",
    "    Args:\n",
    "        lazy_dict: dict with names and LazyFrames of train and test sets.\n",
    "        unq_names: list of unique names of parent datasets.\n",
    "        param_grid: dict of parameters for MLPClassifier.\n",
    "        save_pth: path to save the best model.\n",
    "        cv_threads: number of CPU threads for cross-validation (optional).\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if threads is None:\n",
    "        threads = os.cpu_count() - 2\n",
    "        print(f\"Using {threads} CPU threads!\")\n",
    "        \n",
    "    results = {\n",
    "        \"Dataset Name\": [],\n",
    "        \"Best Recall\": [], \n",
    "        \"Best ROCAUC\": [], \n",
    "        \"Best Accuracy\": [],\n",
    "        \"Fit Time\": []}\n",
    "    \n",
    "    for name in unq_names:\n",
    "        X_train_name = f\"{name}_X_train\"\n",
    "        y_train_name = f\"{name}_y_train\"\n",
    "        (X_test_name, y_test_name) = corr_testset(name)\n",
    "        \n",
    "        X_train = lazy_dict[X_train_name].collect().to_pandas()\n",
    "        y_train = lazy_dict[y_train_name].collect().to_pandas()\n",
    "        X_test = lazy_dict[X_test_name].collect().to_pandas()\n",
    "        y_test = lazy_dict[y_test_name].collect().to_pandas()\n",
    "        \n",
    "        X_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_train.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        X_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        y_test.drop(columns=['__index_level_0__'], errors='ignore', inplace=True)\n",
    "        \n",
    "        y_train = y_train.to_numpy().ravel()\n",
    "        y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_test.shape)\n",
    "\n",
    "        mlp_model = mlp(early_stopping = True, verbose= False, random_state=212)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state= 212)\n",
    "        best_recall = -1\n",
    "        best_model = None\n",
    "        \n",
    "        grid_search = GridSearchCV(mlp_model, param_grid=param_grid, cv=cv, scoring='recall', n_jobs=threads, verbose=1)\n",
    "        \n",
    "        logging.info(f\"Processing dataset: {name}\")\n",
    "        print(f\"Training on {name}...\", flush=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        fit_time = time.time() - start_time\n",
    "\n",
    "        print(f\"GridSearch completed\", flush=True)\n",
    "        logging.info(\"GridSearch completed.\")\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        test_recall = recall_score(y_test, y_pred_test)\n",
    "        test_roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "        \n",
    "        with open(f\"{save_pth}{name}_MLPbaseline\", 'wb') as file:\n",
    "            pickle.dump(best_model, file)\n",
    "        logging.info(f\"Model saved to {save_pth}\")\n",
    "\n",
    "        results[\"Dataset Name\"].append(name)\n",
    "        results[\"Best Recall\"].append(test_recall)\n",
    "        results[\"Best ROCAUC\"].append(test_roc_auc)\n",
    "        results[\"Best Accuracy\"].append(test_accuracy)\n",
    "        results[\"Fit Time\"].append(fit_time)\n",
    "        \n",
    "    pd.DataFrame(results,\"../../Data/GoogleDrive/baseline_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261b5fe8-449c-4e04-ba94-f00585695611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 14 CPU threads!\n",
      "(781716, 121)\n",
      "(781716,)\n",
      "(98675, 121)\n",
      "(98675,)\n",
      "Training on Over_Sample_1:2_threshold_05...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453648, 121)\n",
      "(453648,)\n",
      "(85741, 121)\n",
      "(85741,)\n",
      "Training on Under_Sample_1:1_threshold_01...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2324712, 121)\n",
      "(2324712,)\n",
      "(109919, 121)\n",
      "(109919,)\n",
      "Training on Over_Sample_1:7_threshold_20...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "GridSearch completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtmartinez/anaconda3/envs/203C/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/203C/lib/python3.11/site-packages/IPython/core/magics/execution.py:1340\u001b[0m, in \u001b[0;36mExecutionMagics.time\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1340\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1341\u001b[0m     out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:29\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mmlp_baseline\u001b[0;34m(lazy_dict, unq_names, param_grid, save_pth, threads)\u001b[0m\n\u001b[1;32m     28\u001b[0m (X_test_name, y_test_name) \u001b[38;5;241m=\u001b[39m corr_testset(name)\n\u001b[0;32m---> 30\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mlazy_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     31\u001b[0m y_train \u001b[38;5;241m=\u001b[39m lazy_dict[y_train_name]\u001b[38;5;241m.\u001b[39mcollect()\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/anaconda3/envs/203C/lib/python3.11/site-packages/polars/lazyframe/frame.py:1708\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\u001b[0m\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InProcessQuery(ldf\u001b[38;5;241m.\u001b[39mcollect_concurrently())\n\u001b[0;32m-> 1708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(ldf\u001b[38;5;241m.\u001b[39mcollect())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Log Initialization\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mlogging.basicConfig(filename=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m./log/MLP_baseline.log\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, filemode=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, level=logging.INFO, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    format=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m%(asctime)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(levelname)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m## Paths\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdata_pth = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../Data/GoogleDrive/Encoded_Data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43msave_pth = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../Data/GoogleDrive/Baseline/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m## Read in Parquet to LazyFrame Dictionary.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpq_jar = parquet_to_dict(data_pth)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m## Record the unique dataset names.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43munq_names = unq_df_names(pq_jar)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m## List the parameters. Keep this out of the loop for gridsearching later.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mparam_grid = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mhidden_layer_sizes\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [(8)],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mactivation\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mtanh\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43msolver\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mvalidation_fraction\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [0.2],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madaptive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m], # Not needed for adam\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [0.001],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mlearning_rate_init\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [0.0001],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmax_iter\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [300],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mn_iter_no_change\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [200],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [1000],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [212]}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mresults = mlp_baseline(pq_jar, unq_names, param_grid, save_pth)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# results = mlp_base_parallel(unq_names, pq_jar, params, save_pth)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/203C/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/203C/lib/python3.11/site-packages/IPython/core/magics/execution.py:1347\u001b[0m, in \u001b[0;36mExecutionMagics.time\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(code_2, glob, local_ns)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m end \u001b[38;5;241m=\u001b[39m clock2()\n",
      "File \u001b[0;32m~/anaconda3/envs/203C/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2091\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2088\u001b[0m     msg \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exception_only(etype, value)\n\u001b[1;32m   2089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg)\n\u001b[0;32m-> 2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshowtraceback\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tb_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2092\u001b[0m                   exception_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, running_compiled_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display the exception that just occurred.\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \n\u001b[1;32m   2095\u001b[0m \u001b[38;5;124;03m    If nothing is known about the exception, this is the method which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m    SyntaxError exception, don't try to analyze the stack manually and\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m    simply call this method.\"\"\"\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Log Initialization\n",
    "logging.basicConfig(filename='./log/MLP_baseline.log', filemode='w', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "## Paths\n",
    "data_pth = \"../../Data/GoogleDrive/Encoded_Data/\"\n",
    "save_pth = \"../../Data/GoogleDrive/Baseline/\"\n",
    "\n",
    "## Read in Parquet to LazyFrame Dictionary.\n",
    "pq_jar = parquet_to_dict(data_pth)\n",
    "\n",
    "## Record the unique dataset names.\n",
    "unq_names = unq_df_names(pq_jar)\n",
    "\n",
    "## List the default sklearn mlp_classification parameters.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,)],  # Single layer with 100 neurons\n",
    "    'activation': ['relu'],  # Using 'relu' activation function\n",
    "    'solver': ['adam'],  # Solver set to 'adam'\n",
    "    'alpha': [0.0001],  # L2 penalty (regularization term)\n",
    "    'batch_size': ['auto'],  # 'auto' sets batch size to min(200, n_samples)\n",
    "    'learning_rate': ['constant'],  # Learning rate schedule\n",
    "    'learning_rate_init': [0.001],  # Initial learning rate\n",
    "    'power_t': [0.5],  # The exponent for inverse scaling learning rate\n",
    "    'max_iter': [200],  # Maximum number of iterations\n",
    "    'shuffle': [True],  # Whether to shuffle samples in each iteration\n",
    "    'random_state': [212],  # Random state for reproducibility, can set to a specific number\n",
    "    'tol': [0.0001],  # Tolerance for the optimization\n",
    "    'verbose': [False],  # Whether to print progress messages to stdout\n",
    "    'warm_start': [False],  # Reuse solution of the previous call to fit as initialization\n",
    "    'momentum': [0.9],  # Momentum for gradient descent update\n",
    "    'nesterovs_momentum': [True],  # Whether to use Nesterov's momentum\n",
    "    'early_stopping': [True],  # Whether to use early stopping to terminate training\n",
    "    'validation_fraction': [0.1],  # Proportion of training data to set aside as validation set\n",
    "    'beta_1': [0.9],  # Exponential decay rate for estimates of first moment vector in adam\n",
    "    'beta_2': [0.999],  # Exponential decay rate for estimates of second moment vector in adam\n",
    "    'epsilon': [1e-08],  # Value for numerical stability in adam\n",
    "    'n_iter_no_change': [10],  # Maximum number of epochs to not meet improvement threshold\n",
    "    'max_fun': [15000]  # Maximum number of loss function calls\n",
    "}\n",
    "\n",
    "results = mlp_baseline(pq_jar, unq_names, param_grid, save_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9117de-dd18-479d-a950-064b30e9606b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "203C",
   "language": "python",
   "name": "203c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
