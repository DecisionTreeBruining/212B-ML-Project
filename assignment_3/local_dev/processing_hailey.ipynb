{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets that Drop the Yes or No Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Significant functions from [assignment_3_tools.py](./assignment_3_tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Runtime\n",
    "import pickle # Model Saving\n",
    "import logging # Log Checkpoints\n",
    "import numpy as np # Flatten y vectors\n",
    "import pandas as pd # DataFrame\n",
    "import polars as pl # LazyFrame\n",
    "from sklearn.preprocessing import StandardScaler # X Standardization\n",
    "from sklearn.neural_network import MLPClassifier as mlp # model\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score # Scoring\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from great_tables import GT, md, html, from_column, style, loc, vals\n",
    "from assignment_3_tools import parquet_to_dict, unq_df_names, corr_testset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Datasets and Corresponding Testsets\n",
    "\n",
    "In [preprocess notebook](./taylor_preprocess.ipynb), all of the null-threshold datasets were split into X_train, y_train, X_test, and y_test. The X_train, and y_train sets of each null-threshold datasets were balanced using random over/under sampling. Therefore when `parquet_to_dict()` is called, the dictionary will contain the X_train, y_train, X_test, y_test which correspond to one dataset. To resolve this, `unq_df_names()` and `corr_testset` record the dataset names and corresponding testsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_read_parquet(path):\n",
    "    \"\"\"\n",
    "    Lazy read all parquet files in a folder.\n",
    "    ---\n",
    "    Args: \n",
    "        Path: Relative Path to folder\n",
    "    Return: \n",
    "        lazy_frames_dict: Dictionary of lazy dataframes\n",
    "    \"\"\"\n",
    "    lazy_frames_dict = {}\n",
    "    for filename in os.listdir(path): # Iterate over each file\n",
    "        if filename.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(path, filename) # File Path\n",
    "            lazy_frame = pl.scan_parquet(file_path) # Lazy read\n",
    "            key = os.path.splitext(filename)[0] # key = filename\n",
    "            lazy_frames_dict[key] = lazy_frame # Add lazyframe to dictionary\n",
    "    return lazy_frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../../Data/GoogleDrive/MLP_Dataset/\"\n",
    "df_dict_all = lazy_read_parquet(root_path)\n",
    "X_test = df_dict_all['Under_Sample_1:1_threshold_20_X_test'].collect().to_pandas()\n",
    "X_train = df_dict_all['Under_Sample_1:1_threshold_20_X_train'].collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581178, 121)\n",
      "(109919, 122)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['onehot__State_Alabama', 'onehot__State_Alaska',\n",
       "       'onehot__State_Arizona', 'onehot__State_Arkansas',\n",
       "       'onehot__State_California', 'onehot__State_Colorado',\n",
       "       'onehot__State_Connecticut', 'onehot__State_Delaware',\n",
       "       'onehot__State_District of Columbia', 'onehot__State_Florida',\n",
       "       ...\n",
       "       'LastCheckupTime_label__LastCheckupTime',\n",
       "       'RemovedTeeth_label__RemovedTeeth', 'SmokerStatus_label__SmokerStatus',\n",
       "       'ECigaretteUsage_label__ECigaretteUsage',\n",
       "       'remainder__PhysicalHealthDays', 'remainder__MentalHealthDays',\n",
       "       'remainder__SleepHours', 'remainder__HeightInMeters',\n",
       "       'remainder__WeightInKilograms', '__index_level_0__'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep all the No columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onehot__State_Wyoming\n",
      "onehot__Sex_Male\n",
      "onehot__PhysicalActivities_Yes\n",
      "onehot__HadAsthma_Yes\n",
      "onehot__HadSkinCancer_Yes\n",
      "onehot__HadCOPD_Yes\n",
      "onehot__HadDepressiveDisorder_Yes\n",
      "onehot__HadKidneyDisease_Yes\n",
      "onehot__HadArthritis_Yes\n",
      "onehot__HadDiabetes_Yes, but only during pregnancy (female)\n",
      "onehot__DeafOrHardOfHearing_Yes\n",
      "onehot__BlindOrVisionDifficulty_Yes\n",
      "onehot__DifficultyConcentrating_Yes\n",
      "onehot__DifficultyWalking_Yes\n",
      "onehot__DifficultyDressingBathing_Yes\n",
      "onehot__DifficultyErrands_Yes\n",
      "onehot__ChestScan_Yes\n",
      "onehot__RaceEthnicityCategory_White only, Non-Hispanic\n",
      "onehot__AlcoholDrinkers_Yes\n",
      "onehot__HIVTesting_Yes\n",
      "onehot__FluVaxLast12_Yes\n",
      "onehot__PneumoVaxEver_Yes\n",
      "onehot__TetanusLast10Tdap_Yes, received tetanus shot, but not Tdap\n",
      "onehot__HighRiskLastYear_Yes\n",
      "onehot__CovidPos_Yes\n"
     ]
    }
   ],
   "source": [
    "# Parsing the column names to group by the categorical variable\n",
    "groups = {}\n",
    "for column in X_train.columns:\n",
    "    # format 'onehot__<Category>_<Value>'\n",
    "    category = column.split('__')[1].split('_')[0]\n",
    "    if category in groups:\n",
    "        groups[category].append(column)\n",
    "    else:\n",
    "        groups[category] = [column]\n",
    "\n",
    "# Drop the first column from each group\n",
    "for category, columns in groups.items():\n",
    "    # Drop the last column (Yes columns) to reduce sparsity in feature space\n",
    "    if len(columns) != 1:\n",
    "        print(columns[-1])\n",
    "        X_train.drop(columns=columns[-1], inplace=True)\n",
    "        X_test.drop(columns=columns[-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581178, 120)\n",
      "(109919, 121)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_No.shape)\n",
    "print(X_test_No.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_parquet(root_path + \"Under_Sample_1:1_threshold_20_X_train_No.parquet\")\n",
    "X_test.to_parquet(root_path + \"Under_Sample_1:1_threshold_20_X_test_No.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep all the Yes columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the column names to group by the categorical variable\n",
    "groups = {}\n",
    "for column in X_train.columns:\n",
    "    # format 'onehot__<Category>_<Value>'\n",
    "    category = column.split('__')[1].split('_')[0]\n",
    "    if category in groups:\n",
    "        groups[category].append(column)\n",
    "    else:\n",
    "        groups[category] = [column]\n",
    "\n",
    "# Drop the first column from each group\n",
    "for category, columns in groups.items():\n",
    "    # Drop the first column (No columns)\n",
    "    if len(columns) != 1:\n",
    "        X_train.drop(columns=columns[0], inplace=True)\n",
    "        X_test.drop(columns=columns[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['onehot__State_Alaska', 'onehot__State_Arizona',\n",
       "       'onehot__State_Arkansas', 'onehot__State_California',\n",
       "       'onehot__State_Colorado', 'onehot__State_Connecticut',\n",
       "       'onehot__State_Delaware', 'onehot__State_District of Columbia',\n",
       "       'onehot__State_Florida', 'onehot__State_Georgia', 'onehot__State_Guam',\n",
       "       'onehot__State_Hawaii', 'onehot__State_Idaho', 'onehot__State_Illinois',\n",
       "       'onehot__State_Indiana', 'onehot__State_Iowa', 'onehot__State_Kansas',\n",
       "       'onehot__State_Kentucky', 'onehot__State_Louisiana',\n",
       "       'onehot__State_Maine', 'onehot__State_Maryland',\n",
       "       'onehot__State_Massachusetts', 'onehot__State_Michigan',\n",
       "       'onehot__State_Minnesota', 'onehot__State_Mississippi',\n",
       "       'onehot__State_Missouri', 'onehot__State_Montana',\n",
       "       'onehot__State_Nebraska', 'onehot__State_Nevada',\n",
       "       'onehot__State_New Hampshire', 'onehot__State_New Jersey',\n",
       "       'onehot__State_New Mexico', 'onehot__State_New York',\n",
       "       'onehot__State_North Carolina', 'onehot__State_North Dakota',\n",
       "       'onehot__State_Ohio', 'onehot__State_Oklahoma', 'onehot__State_Oregon',\n",
       "       'onehot__State_Pennsylvania', 'onehot__State_Puerto Rico',\n",
       "       'onehot__State_Rhode Island', 'onehot__State_South Carolina',\n",
       "       'onehot__State_South Dakota', 'onehot__State_Tennessee',\n",
       "       'onehot__State_Texas', 'onehot__State_Utah', 'onehot__State_Vermont',\n",
       "       'onehot__State_Virgin Islands', 'onehot__State_Virginia',\n",
       "       'onehot__State_Washington', 'onehot__State_West Virginia',\n",
       "       'onehot__State_Wisconsin', 'onehot__State_Wyoming', 'onehot__Sex_Male',\n",
       "       'onehot__PhysicalActivities_Yes', 'onehot__HadAsthma_Yes',\n",
       "       'onehot__HadSkinCancer_Yes', 'onehot__HadCOPD_Yes',\n",
       "       'onehot__HadDepressiveDisorder_Yes', 'onehot__HadKidneyDisease_Yes',\n",
       "       'onehot__HadArthritis_Yes',\n",
       "       'onehot__HadDiabetes_No, pre-diabetes or borderline diabetes',\n",
       "       'onehot__HadDiabetes_Yes',\n",
       "       'onehot__HadDiabetes_Yes, but only during pregnancy (female)',\n",
       "       'onehot__DeafOrHardOfHearing_Yes',\n",
       "       'onehot__BlindOrVisionDifficulty_Yes',\n",
       "       'onehot__DifficultyConcentrating_Yes', 'onehot__DifficultyWalking_Yes',\n",
       "       'onehot__DifficultyDressingBathing_Yes',\n",
       "       'onehot__DifficultyErrands_Yes', 'onehot__ChestScan_Yes',\n",
       "       'onehot__RaceEthnicityCategory_Hispanic',\n",
       "       'onehot__RaceEthnicityCategory_Multiracial, Non-Hispanic',\n",
       "       'onehot__RaceEthnicityCategory_Other race only, Non-Hispanic',\n",
       "       'onehot__RaceEthnicityCategory_White only, Non-Hispanic',\n",
       "       'onehot__AlcoholDrinkers_Yes', 'onehot__HIVTesting_Yes',\n",
       "       'onehot__FluVaxLast12_Yes', 'onehot__PneumoVaxEver_Yes',\n",
       "       'onehot__TetanusLast10Tdap_Yes, received Tdap',\n",
       "       'onehot__TetanusLast10Tdap_Yes, received tetanus shot but not sure what type',\n",
       "       'onehot__TetanusLast10Tdap_Yes, received tetanus shot, but not Tdap',\n",
       "       'onehot__HighRiskLastYear_Yes',\n",
       "       'onehot__CovidPos_Tested positive using home test without a health professional',\n",
       "       'onehot__CovidPos_Yes', 'label__AgeCategory',\n",
       "       'GeneralHealth_label__GeneralHealth',\n",
       "       'LastCheckupTime_label__LastCheckupTime',\n",
       "       'RemovedTeeth_label__RemovedTeeth', 'SmokerStatus_label__SmokerStatus',\n",
       "       'ECigaretteUsage_label__ECigaretteUsage',\n",
       "       'remainder__PhysicalHealthDays', 'remainder__MentalHealthDays',\n",
       "       'remainder__SleepHours', 'remainder__HeightInMeters',\n",
       "       'remainder__WeightInKilograms'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_parquet(root_path + \"Under_Sample_1:1_threshold_20_X_train_Yes.parquet\")\n",
    "X_test.to_parquet(root_path + \"Under_Sample_1:1_threshold_20_X_test_Yes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in model results pickle files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recall   ROC_AUC  Accuracy\n",
       "0  0.705783  0.707385  0.662288"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = \"../../Data/GoogleDrive/MLP_Results/\"\n",
    "xgb_results = pd.read_parquet(root_path + \"xgb_results.parquet\")\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP models test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Grid_Variable</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate_init</td>\n",
       "      <td>{'learning_rate_init': 0.01}</td>\n",
       "      <td>0.767905</td>\n",
       "      <td>0.816309</td>\n",
       "      <td>0.730010</td>\n",
       "      <td>72.824502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>{'max_iter': 100}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>105.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>{'batch_size': 100}</td>\n",
       "      <td>0.736729</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.742620</td>\n",
       "      <td>150.978772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>iter_no_change</td>\n",
       "      <td>{'n_iter_no_change': 100}</td>\n",
       "      <td>0.712601</td>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.745294</td>\n",
       "      <td>399.201779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_best_params</td>\n",
       "      <td>{'hidden_layer_sizes': [47, 46, 46, 46], 'lear...</td>\n",
       "      <td>0.811796</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.691627</td>\n",
       "      <td>162.356431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_bayes_params</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.000523565603...</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "      <td>49.996617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>momentum</td>\n",
       "      <td>{'momentum': 0.0}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>108.784148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>neurons</td>\n",
       "      <td>{'hidden_layer_sizes': 500}</td>\n",
       "      <td>0.448487</td>\n",
       "      <td>0.729403</td>\n",
       "      <td>0.795750</td>\n",
       "      <td>379.136686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>alpha</td>\n",
       "      <td>{'alpha': 0.0}</td>\n",
       "      <td>0.719418</td>\n",
       "      <td>0.811282</td>\n",
       "      <td>0.745786</td>\n",
       "      <td>101.764314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>layers</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 100, 100, 100]}</td>\n",
       "      <td>0.518116</td>\n",
       "      <td>0.760817</td>\n",
       "      <td>0.809887</td>\n",
       "      <td>832.888808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>{'learning_rate': 'constant'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>96.904531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>baseline</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>67.248320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>activation</td>\n",
       "      <td>{'activation': 'logistic'}</td>\n",
       "      <td>0.660207</td>\n",
       "      <td>0.793378</td>\n",
       "      <td>0.753255</td>\n",
       "      <td>154.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>solver</td>\n",
       "      <td>{'solver': 'adam'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>76.121285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_best_params_vem_2</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.694148</td>\n",
       "      <td>162.822526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dataset_Name       Grid_Variable  \\\n",
       "0   Under_Sample_1:1_threshold_20  learning_rate_init   \n",
       "1   Under_Sample_1:1_threshold_20            max_iter   \n",
       "2   Under_Sample_1:1_threshold_20          batch_size   \n",
       "3   Under_Sample_1:1_threshold_20      iter_no_change   \n",
       "4   Under_Sample_1:1_threshold_20        _best_params   \n",
       "5   Under_Sample_1:1_threshold_20       _bayes_params   \n",
       "6   Under_Sample_1:1_threshold_20            momentum   \n",
       "7   Under_Sample_1:1_threshold_20             neurons   \n",
       "8   Under_Sample_1:1_threshold_20               alpha   \n",
       "9   Under_Sample_1:1_threshold_20              layers   \n",
       "10  Under_Sample_1:1_threshold_20       learning_rate   \n",
       "11  Under_Sample_1:1_threshold_20            baseline   \n",
       "12  Under_Sample_1:1_threshold_20          activation   \n",
       "13  Under_Sample_1:1_threshold_20              solver   \n",
       "14  Under_Sample_1:1_threshold_20  _best_params_vem_2   \n",
       "15                            NaN                 NaN   \n",
       "\n",
       "                                           Parameters    Recall   ROC_AUC  \\\n",
       "0                        {'learning_rate_init': 0.01}  0.767905  0.816309   \n",
       "1                                   {'max_iter': 100}  0.752738  0.814254   \n",
       "2                                 {'batch_size': 100}  0.736729  0.815755   \n",
       "3                           {'n_iter_no_change': 100}  0.712601  0.808466   \n",
       "4   {'hidden_layer_sizes': [47, 46, 46, 46], 'lear...  0.811796  0.814387   \n",
       "5   {'activation': 'relu', 'alpha': 0.000523565603...  0.770816  0.813239   \n",
       "6                                   {'momentum': 0.0}  0.752738  0.814254   \n",
       "7                         {'hidden_layer_sizes': 500}  0.448487  0.729403   \n",
       "8                                      {'alpha': 0.0}  0.719418  0.811282   \n",
       "9   {'hidden_layer_sizes': [100, 100, 100, 100, 100]}  0.518116  0.760817   \n",
       "10                      {'learning_rate': 'constant'}  0.752738  0.814254   \n",
       "11  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.752738  0.814254   \n",
       "12                         {'activation': 'logistic'}  0.660207  0.793378   \n",
       "13                                 {'solver': 'adam'}  0.752738  0.814254   \n",
       "14  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.815167  0.815958   \n",
       "15                                                NaN  0.705783  0.707385   \n",
       "\n",
       "    Accuracy    Fit_Time  \n",
       "0   0.730010   72.824502  \n",
       "1   0.732085  105.687169  \n",
       "2   0.742620  150.978772  \n",
       "3   0.745294  399.201779  \n",
       "4   0.691627  162.356431  \n",
       "5   0.722086   49.996617  \n",
       "6   0.732085  108.784148  \n",
       "7   0.795750  379.136686  \n",
       "8   0.745786  101.764314  \n",
       "9   0.809887  832.888808  \n",
       "10  0.732085   96.904531  \n",
       "11  0.732085   67.248320  \n",
       "12  0.753255  154.270200  \n",
       "13  0.732085   76.121285  \n",
       "14  0.694148  162.822526  \n",
       "15  0.662288         NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use glob to find all parquet files starting with 'test_results' in the directory\n",
    "parquet_files = glob.glob(root_path + 'test_results*.parquet')\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "model_results = []\n",
    "\n",
    "# Loop through the list of parquet files and read each one into a DataFrame\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    model_results.append(df)\n",
    "\n",
    "# add in xgb result to the list\n",
    "model_results.append(xgb_results)\n",
    "\n",
    "# Optionally, concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(model_results, ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Grid_Variable</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate_init</td>\n",
       "      <td>{'learning_rate_init': 0.01}</td>\n",
       "      <td>0.767905</td>\n",
       "      <td>0.816309</td>\n",
       "      <td>0.730010</td>\n",
       "      <td>72.824502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>{'max_iter': 100}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>105.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>{'batch_size': 100}</td>\n",
       "      <td>0.736729</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.742620</td>\n",
       "      <td>150.978772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>iter_no_change</td>\n",
       "      <td>{'n_iter_no_change': 100}</td>\n",
       "      <td>0.712601</td>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.745294</td>\n",
       "      <td>399.201779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_best_params</td>\n",
       "      <td>{'hidden_layer_sizes': [47, 46, 46, 46], 'lear...</td>\n",
       "      <td>0.811796</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.691627</td>\n",
       "      <td>162.356431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>bayes</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.000523565603...</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "      <td>49.996617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>momentum</td>\n",
       "      <td>{'momentum': 0.0}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>108.784148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>neurons</td>\n",
       "      <td>{'hidden_layer_sizes': 500}</td>\n",
       "      <td>0.448487</td>\n",
       "      <td>0.729403</td>\n",
       "      <td>0.795750</td>\n",
       "      <td>379.136686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>alpha</td>\n",
       "      <td>{'alpha': 0.0}</td>\n",
       "      <td>0.719418</td>\n",
       "      <td>0.811282</td>\n",
       "      <td>0.745786</td>\n",
       "      <td>101.764314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>layers</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 100, 100, 100]}</td>\n",
       "      <td>0.518116</td>\n",
       "      <td>0.760817</td>\n",
       "      <td>0.809887</td>\n",
       "      <td>832.888808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>{'learning_rate': 'constant'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>96.904531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>baseline</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>67.248320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>activation</td>\n",
       "      <td>{'activation': 'logistic'}</td>\n",
       "      <td>0.660207</td>\n",
       "      <td>0.793378</td>\n",
       "      <td>0.753255</td>\n",
       "      <td>154.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>solver</td>\n",
       "      <td>{'solver': 'adam'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>76.121285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>frank_results</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.694148</td>\n",
       "      <td>162.822526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>xgb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dataset_Name       Grid_Variable  \\\n",
       "0   Under_Sample_1:1_threshold_20  learning_rate_init   \n",
       "1   Under_Sample_1:1_threshold_20            max_iter   \n",
       "2   Under_Sample_1:1_threshold_20          batch_size   \n",
       "3   Under_Sample_1:1_threshold_20      iter_no_change   \n",
       "4   Under_Sample_1:1_threshold_20        _best_params   \n",
       "5   Under_Sample_1:1_threshold_20               bayes   \n",
       "6   Under_Sample_1:1_threshold_20            momentum   \n",
       "7   Under_Sample_1:1_threshold_20             neurons   \n",
       "8   Under_Sample_1:1_threshold_20               alpha   \n",
       "9   Under_Sample_1:1_threshold_20              layers   \n",
       "10  Under_Sample_1:1_threshold_20       learning_rate   \n",
       "11  Under_Sample_1:1_threshold_20            baseline   \n",
       "12  Under_Sample_1:1_threshold_20          activation   \n",
       "13  Under_Sample_1:1_threshold_20              solver   \n",
       "14  Under_Sample_1:1_threshold_20       frank_results   \n",
       "15  Under_Sample_1:1_threshold_20                 xgb   \n",
       "\n",
       "                                           Parameters    Recall   ROC_AUC  \\\n",
       "0                        {'learning_rate_init': 0.01}  0.767905  0.816309   \n",
       "1                                   {'max_iter': 100}  0.752738  0.814254   \n",
       "2                                 {'batch_size': 100}  0.736729  0.815755   \n",
       "3                           {'n_iter_no_change': 100}  0.712601  0.808466   \n",
       "4   {'hidden_layer_sizes': [47, 46, 46, 46], 'lear...  0.811796  0.814387   \n",
       "5   {'activation': 'relu', 'alpha': 0.000523565603...  0.770816  0.813239   \n",
       "6                                   {'momentum': 0.0}  0.752738  0.814254   \n",
       "7                         {'hidden_layer_sizes': 500}  0.448487  0.729403   \n",
       "8                                      {'alpha': 0.0}  0.719418  0.811282   \n",
       "9   {'hidden_layer_sizes': [100, 100, 100, 100, 100]}  0.518116  0.760817   \n",
       "10                      {'learning_rate': 'constant'}  0.752738  0.814254   \n",
       "11  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.752738  0.814254   \n",
       "12                         {'activation': 'logistic'}  0.660207  0.793378   \n",
       "13                                 {'solver': 'adam'}  0.752738  0.814254   \n",
       "14  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.815167  0.815958   \n",
       "15                                                NaN  0.705783  0.707385   \n",
       "\n",
       "    Accuracy    Fit_Time  \n",
       "0   0.730010   72.824502  \n",
       "1   0.732085  105.687169  \n",
       "2   0.742620  150.978772  \n",
       "3   0.745294  399.201779  \n",
       "4   0.691627  162.356431  \n",
       "5   0.722086   49.996617  \n",
       "6   0.732085  108.784148  \n",
       "7   0.795750  379.136686  \n",
       "8   0.745786  101.764314  \n",
       "9   0.809887  832.888808  \n",
       "10  0.732085   96.904531  \n",
       "11  0.732085   67.248320  \n",
       "12  0.753255  154.270200  \n",
       "13  0.732085   76.121285  \n",
       "14  0.694148  162.822526  \n",
       "15  0.662288         NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[15, 0] = 'Under_Sample_1:1_threshold_20'\n",
    "combined_df.iloc[15, 1] = 'xgb'\n",
    "combined_df.iloc[14, 1] = 'frank_results'\n",
    "combined_df.iloc[5, 1] = 'bayes'\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Grid_Variable</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate_init</td>\n",
       "      <td>{'learning_rate_init': 0.01}</td>\n",
       "      <td>0.767905</td>\n",
       "      <td>0.816309</td>\n",
       "      <td>0.730010</td>\n",
       "      <td>72.824502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>{'max_iter': 100}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>105.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>{'batch_size': 100}</td>\n",
       "      <td>0.736729</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.742620</td>\n",
       "      <td>150.978772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>iter_no_change</td>\n",
       "      <td>{'n_iter_no_change': 100}</td>\n",
       "      <td>0.712601</td>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.745294</td>\n",
       "      <td>399.201779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>_best_params</td>\n",
       "      <td>{'hidden_layer_sizes': [47, 46, 46, 46], 'lear...</td>\n",
       "      <td>0.811796</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.691627</td>\n",
       "      <td>162.356431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>bayes</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.000523565603...</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>0.722086</td>\n",
       "      <td>49.996617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>momentum</td>\n",
       "      <td>{'momentum': 0.0}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>108.784148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>neurons</td>\n",
       "      <td>{'hidden_layer_sizes': 500}</td>\n",
       "      <td>0.448487</td>\n",
       "      <td>0.729403</td>\n",
       "      <td>0.795750</td>\n",
       "      <td>379.136686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>alpha</td>\n",
       "      <td>{'alpha': 0.0}</td>\n",
       "      <td>0.719418</td>\n",
       "      <td>0.811282</td>\n",
       "      <td>0.745786</td>\n",
       "      <td>101.764314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>layers</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 100, 100, 100]}</td>\n",
       "      <td>0.518116</td>\n",
       "      <td>0.760817</td>\n",
       "      <td>0.809887</td>\n",
       "      <td>832.888808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>{'learning_rate': 'constant'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>96.904531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>baseline</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>67.248320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>activation</td>\n",
       "      <td>{'activation': 'logistic'}</td>\n",
       "      <td>0.660207</td>\n",
       "      <td>0.793378</td>\n",
       "      <td>0.753255</td>\n",
       "      <td>154.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>solver</td>\n",
       "      <td>{'solver': 'adam'}</td>\n",
       "      <td>0.752738</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>76.121285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>frank_results</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.694148</td>\n",
       "      <td>162.822526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Under_Sample_1:1_threshold_20</td>\n",
       "      <td>xgb</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>0.662288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dataset_Name       Grid_Variable  \\\n",
       "0   Under_Sample_1:1_threshold_20  learning_rate_init   \n",
       "1   Under_Sample_1:1_threshold_20            max_iter   \n",
       "2   Under_Sample_1:1_threshold_20          batch_size   \n",
       "3   Under_Sample_1:1_threshold_20      iter_no_change   \n",
       "4   Under_Sample_1:1_threshold_20        _best_params   \n",
       "5   Under_Sample_1:1_threshold_20               bayes   \n",
       "6   Under_Sample_1:1_threshold_20            momentum   \n",
       "7   Under_Sample_1:1_threshold_20             neurons   \n",
       "8   Under_Sample_1:1_threshold_20               alpha   \n",
       "9   Under_Sample_1:1_threshold_20              layers   \n",
       "10  Under_Sample_1:1_threshold_20       learning_rate   \n",
       "11  Under_Sample_1:1_threshold_20            baseline   \n",
       "12  Under_Sample_1:1_threshold_20          activation   \n",
       "13  Under_Sample_1:1_threshold_20              solver   \n",
       "14  Under_Sample_1:1_threshold_20       frank_results   \n",
       "15  Under_Sample_1:1_threshold_20                 xgb   \n",
       "\n",
       "                                           Parameters    Recall   ROC_AUC  \\\n",
       "0                        {'learning_rate_init': 0.01}  0.767905  0.816309   \n",
       "1                                   {'max_iter': 100}  0.752738  0.814254   \n",
       "2                                 {'batch_size': 100}  0.736729  0.815755   \n",
       "3                           {'n_iter_no_change': 100}  0.712601  0.808466   \n",
       "4   {'hidden_layer_sizes': [47, 46, 46, 46], 'lear...  0.811796  0.814387   \n",
       "5   {'activation': 'relu', 'alpha': 0.000523565603...  0.770816  0.813239   \n",
       "6                                   {'momentum': 0.0}  0.752738  0.814254   \n",
       "7                         {'hidden_layer_sizes': 500}  0.448487  0.729403   \n",
       "8                                      {'alpha': 0.0}  0.719418  0.811282   \n",
       "9   {'hidden_layer_sizes': [100, 100, 100, 100, 100]}  0.518116  0.760817   \n",
       "10                      {'learning_rate': 'constant'}  0.752738  0.814254   \n",
       "11  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.752738  0.814254   \n",
       "12                         {'activation': 'logistic'}  0.660207  0.793378   \n",
       "13                                 {'solver': 'adam'}  0.752738  0.814254   \n",
       "14  {'activation': 'relu', 'alpha': 0.0001, 'batch...  0.815167  0.815958   \n",
       "15                                                 {}  0.705783  0.707385   \n",
       "\n",
       "    Accuracy    Fit_Time  \n",
       "0   0.730010   72.824502  \n",
       "1   0.732085  105.687169  \n",
       "2   0.742620  150.978772  \n",
       "3   0.745294  399.201779  \n",
       "4   0.691627  162.356431  \n",
       "5   0.722086   49.996617  \n",
       "6   0.732085  108.784148  \n",
       "7   0.795750  379.136686  \n",
       "8   0.745786  101.764314  \n",
       "9   0.809887  832.888808  \n",
       "10  0.732085   96.904531  \n",
       "11  0.732085   67.248320  \n",
       "12  0.753255  154.270200  \n",
       "13  0.732085   76.121285  \n",
       "14  0.694148  162.822526  \n",
       "15  0.662288    0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['Parameters'].fillna({}, inplace=True)\n",
    "combined_df['Fit_Time'].fillna(0, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(columns=['Parameters'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_parquet(root_path + \"test_results-combined.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
