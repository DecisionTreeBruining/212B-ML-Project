{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, auc, _scorer\n",
    ")\n",
    "from sklearn.tree import export_graphviz\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier as xgbclass\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "\n",
    "root_path = \"../../Data/GoogleDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from parquet files\n",
    "X_train = pd.read_parquet(root_path + \"X_train.parquet\")\n",
    "X_test = pd.read_parquet(root_path + \"X_test.parquet\")\n",
    "y_train = pd.read_parquet(root_path + \"y_train.parquet\")\n",
    "y_test = pd.read_parquet(root_path + \"y_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ucla/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Best AUC ROC Score: 0.8395325813034636\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=69)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 600, 1000],\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='roc_auc', \n",
    "                           n_jobs=8) # Parallel\n",
    "\n",
    "y_train = y_train.to_numpy().ravel()\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "#grid_search.fit(X_train, y_train)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_rf_params = grid_search.best_params_\n",
    "best_rf_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_rf_params)\n",
    "print(\"Best AUC ROC Score:\", best_rf_score)\n",
    "\n",
    "# Use the best model to make predictions on the testing data\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "rf_predcitions_prob = best_rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# execution_time = end_time - start_time\n",
    "# print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "with open(root_path + \"rf_model_2.pkl\", 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.801519936833794\n",
      "Random Forest Precision: 0.6545293072824157\n",
      "Random Forest Recall: 0.43643900513225425\n",
      "Random Forest F1 Score: 0.5236854571293226\n",
      "Random Forest AUC ROC: 0.840629048578763\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.92      0.87     30396\n",
      "         1.0       0.65      0.44      0.52     10132\n",
      "\n",
      "    accuracy                           0.80     40528\n",
      "   macro avg       0.74      0.68      0.70     40528\n",
      "weighted avg       0.79      0.80      0.79     40528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Precision\n",
    "rf_precision = precision_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Precision:\", rf_precision)\n",
    "\n",
    "# Recall\n",
    "rf_recall = recall_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Recall:\", rf_recall)\n",
    "\n",
    "# F1 Score\n",
    "rf_f1 = f1_score(y_test, rf_predictions)\n",
    "print(\"Random Forest F1 Score:\", rf_f1)\n",
    "\n",
    "# AUC ROC\n",
    "rf_auc_roc = roc_auc_score(y_test, rf_predcitions_prob)\n",
    "print(\"Random Forest AUC ROC:\", rf_auc_roc)\n",
    "\n",
    "# Classification Report\n",
    "rf_classification_report = classification_report(y_test, rf_predictions)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(rf_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new models from pickles\n",
    "with open(root_path + \"rf_model_fixed.pkl\", 'rb') as file:\n",
    "    rf_model_fixed = pickle.load(file)\n",
    "\n",
    "with open(root_path + \"xgboost_model_fixed.pkl\", 'rb') as file:\n",
    "    xgboost_model_fixed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest:\n",
      "{'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Best Parameters for XGBoost:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# Best parameters for Random Forest\n",
    "print(\"Best Parameters for Random Forest:\")\n",
    "print(rf_model_fixed.best_params_)\n",
    "\n",
    "# Best parameters for XGBoost\n",
    "print(\"Best Parameters for XGBoost:\")\n",
    "print(xgboost_model_fixed.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.8395430319778918, 'Precision': 0.7378424433084284, 'Recall': 0.5555665219107777, 'F1 Score': 0.6338607060413265, 'AUC ROC': 0.8963825743377312}\n",
      "{'Accuracy': 0.8046535728385313, 'Precision': 0.6440744113438273, 'Recall': 0.4886498223450454, 'F1 Score': 0.5556989730063415, 'AUC ROC': 0.8478938048682513}\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the fixed models\n",
    "rf_predictions_fixed = rf_model_fixed.predict(X_test)\n",
    "rf_predcitions_prob_fixed = rf_model_fixed.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rf_accuracy_fixed = accuracy_score(y_test, rf_predictions_fixed)\n",
    "rf_precision_fixed = precision_score(y_test, rf_predictions_fixed)\n",
    "rf_recall_fixed = recall_score(y_test, rf_predictions_fixed)\n",
    "rf_f1_fixed = f1_score(y_test, rf_predictions_fixed)\n",
    "rf_auc_roc_fixed = roc_auc_score(y_test, rf_predcitions_prob_fixed)\n",
    "rf_classification_report_fixed = classification_report(y_test, rf_predictions_fixed)\n",
    "\n",
    "xgboost_predictions_fixed = xgboost_model_fixed.predict(X_test)\n",
    "xgboost_predcitions_prob_fixed = xgboost_model_fixed.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgboost_accuracy_fixed = accuracy_score(y_test, xgboost_predictions_fixed)\n",
    "xgboost_precision_fixed = precision_score(y_test, xgboost_predictions_fixed)\n",
    "xgboost_recall_fixed = recall_score(y_test, xgboost_predictions_fixed)\n",
    "xgboost_f1_fixed = f1_score(y_test, xgboost_predictions_fixed)\n",
    "xgboost_auc_roc_fixed = roc_auc_score(y_test, xgboost_predcitions_prob_fixed)\n",
    "xgboost_classification_report_fixed = classification_report(y_test, xgboost_predictions_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.93      0.90     30396\n",
      "         1.0       0.74      0.56      0.63     10132\n",
      "\n",
      "    accuracy                           0.84     40528\n",
      "   macro avg       0.80      0.74      0.77     40528\n",
      "weighted avg       0.83      0.84      0.83     40528\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87     30396\n",
      "         1.0       0.64      0.49      0.56     10132\n",
      "\n",
      "    accuracy                           0.80     40528\n",
      "   macro avg       0.74      0.70      0.72     40528\n",
      "weighted avg       0.79      0.80      0.80     40528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rf_classification_report_fixed)\n",
    "print(xgboost_classification_report_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Accuracy  Precision    Recall  F1 Score   AUC ROC\n",
      "Random Forest  0.839543   0.737842  0.555567  0.633861  0.896383\n",
      "         Accuracy  Precision   Recall  F1 Score   AUC ROC\n",
      "XGBoost  0.804654   0.644074  0.48865  0.555699  0.847894\n"
     ]
    }
   ],
   "source": [
    "# Seperate tables for the metrics of the models fixed \n",
    "\n",
    "# Random Forest\n",
    "rf_metrics_fixed = {\n",
    "    \"Accuracy\": rf_accuracy_fixed,\n",
    "    \"Precision\": rf_precision_fixed,\n",
    "    \"Recall\": rf_recall_fixed,\n",
    "    \"F1 Score\": rf_f1_fixed,\n",
    "    \"AUC ROC\": rf_auc_roc_fixed\n",
    "}\n",
    "rf_metrics_fixed_df = pd.DataFrame(rf_metrics_fixed, index=[\"Random Forest\"])\n",
    "print(rf_metrics_fixed_df)\n",
    "\n",
    "# XGBoost\n",
    "xgboost_metrics_fixed = {\n",
    "    \"Accuracy\": xgboost_accuracy_fixed,\n",
    "    \"Precision\": xgboost_precision_fixed,\n",
    "    \"Recall\": xgboost_recall_fixed,\n",
    "    \"F1 Score\": xgboost_f1_fixed,\n",
    "    \"AUC ROC\": xgboost_auc_roc_fixed\n",
    "}\n",
    "xgboost_metrics_fixed_df = pd.DataFrame(xgboost_metrics_fixed, index=[\"XGBoost\"])\n",
    "print(xgboost_metrics_fixed_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
